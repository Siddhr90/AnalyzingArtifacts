{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x02efG2VUuMt",
    "outputId": "1533d664-1939-42f5-f84e-660af251525d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# import google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Gp2ZeuFTwaM_",
    "outputId": "7546a252-3155-4011-e570-fef4602bc1ff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that you are in drive\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMVG7vKwSZ02",
    "outputId": "3c407acb-533e-458b-a7ca-f2b42d5a9e7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/NLP_FINAL/starter_code\n"
     ]
    }
   ],
   "source": [
    "# change to to required working directory\n",
    "cd /content/drive/MyDrive/NLP_FINAL/starter_code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNrdPGcAR5vs",
    "outputId": "956e6828-2ad1-4e27-897b-2f8af8d5c51c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate (from -r requirements.txt (line 1))\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets (from -r requirements.txt (line 2))\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.66.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.35.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (9.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets->-r requirements.txt (line 2))\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets->-r requirements.txt (line 2))\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.4.1)\n",
      "Collecting multiprocess (from datasets->-r requirements.txt (line 2))\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (0.15.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3.post1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\n",
      "Installing collected packages: pyarrow-hotfix, dill, multiprocess, accelerate, datasets\n",
      "Successfully installed accelerate-0.25.0 datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n"
     ]
    }
   ],
   "source": [
    "# install  requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "1J90l9Tfw1VT",
    "outputId": "15374c5f-f1fe-4598-e2f4-5132d39da52d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-11 07:39:30.995941: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-11 07:39:30.996005: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-11 07:39:30.996043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-11 07:39:32.320415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading builder script: 100% 3.82k/3.82k [00:00<00:00, 24.8MB/s]\n",
      "Downloading metadata: 100% 1.90k/1.90k [00:00<00:00, 12.8MB/s]\n",
      "Downloading readme: 100% 14.1k/14.1k [00:00<00:00, 40.6MB/s]\n",
      "Downloading: 100% 1.93k/1.93k [00:00<00:00, 12.3MB/s]\n",
      "Downloading: 100% 1.26M/1.26M [00:00<00:00, 4.71MB/s]\n",
      "Downloading: 100% 65.9M/65.9M [00:01<00:00, 47.6MB/s]\n",
      "Downloading: 100% 1.26M/1.26M [00:00<00:00, 4.83MB/s]\n",
      "Downloading (…)lve/main/config.json: 100% 665/665 [00:00<00:00, 3.71MB/s]\n",
      "Downloading pytorch_model.bin: 100% 54.2M/54.2M [00:00<00:00, 80.0MB/s]\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (…)okenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 179kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 2.99MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 3.50MB/s]\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Filter: 100% 10000/10000 [00:00<00:00, 106765.77 examples/s]\n",
      "Filter: 100% 550152/550152 [00:02<00:00, 218924.71 examples/s]\n",
      "Filter: 100% 10000/10000 [00:00<00:00, 336756.64 examples/s]\n",
      "Map (num_proc=2): 100% 549367/549367 [01:07<00:00, 8156.59 examples/s] \n",
      "  0% 0/206013 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.9336, 'learning_rate': 4.987864843480751e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7437, 'learning_rate': 4.975729686961503e-05, 'epoch': 0.01}\n",
      "{'loss': 0.6869, 'learning_rate': 4.963594530442254e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6444, 'learning_rate': 4.951459373923005e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6248, 'learning_rate': 4.9393242174037564e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6085, 'learning_rate': 4.927189060884507e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6122, 'learning_rate': 4.9150539043652585e-05, 'epoch': 0.05}\n",
      "{'loss': 0.592, 'learning_rate': 4.90291874784601e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5958, 'learning_rate': 4.890783591326761e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5871, 'learning_rate': 4.8786484348075126e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5761, 'learning_rate': 4.866513278288264e-05, 'epoch': 0.08}\n",
      "{'loss': 0.5814, 'learning_rate': 4.8543781217690146e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5822, 'learning_rate': 4.842242965249766e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5619, 'learning_rate': 4.8301078087305174e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5404, 'learning_rate': 4.817972652211268e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5551, 'learning_rate': 4.80583749569202e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5605, 'learning_rate': 4.793702339172771e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5306, 'learning_rate': 4.781567182653522e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5494, 'learning_rate': 4.7694320261342736e-05, 'epoch': 0.14}\n",
      "{'loss': 0.5351, 'learning_rate': 4.757296869615024e-05, 'epoch': 0.15}\n",
      "{'loss': 0.5199, 'learning_rate': 4.7451617130957756e-05, 'epoch': 0.15}\n",
      "{'loss': 0.5446, 'learning_rate': 4.733026556576527e-05, 'epoch': 0.16}\n",
      "{'loss': 0.5316, 'learning_rate': 4.720891400057278e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5343, 'learning_rate': 4.70875624353803e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5292, 'learning_rate': 4.6966210870187804e-05, 'epoch': 0.18}\n",
      "{'loss': 0.5343, 'learning_rate': 4.684485930499532e-05, 'epoch': 0.19}\n",
      "{'loss': 0.5187, 'learning_rate': 4.672350773980283e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5071, 'learning_rate': 4.660215617461034e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5158, 'learning_rate': 4.648080460941785e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5111, 'learning_rate': 4.6359453044225366e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4955, 'learning_rate': 4.623810147903288e-05, 'epoch': 0.23}\n",
      "{'loss': 0.5119, 'learning_rate': 4.611674991384039e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4996, 'learning_rate': 4.599539834864791e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5203, 'learning_rate': 4.5874046783455414e-05, 'epoch': 0.25}\n",
      "{'loss': 0.5287, 'learning_rate': 4.575269521826293e-05, 'epoch': 0.25}\n",
      "{'loss': 0.5091, 'learning_rate': 4.563134365307044e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4844, 'learning_rate': 4.550999208787795e-05, 'epoch': 0.27}\n",
      "{'loss': 0.5027, 'learning_rate': 4.538864052268547e-05, 'epoch': 0.28}\n",
      "{'loss': 0.495, 'learning_rate': 4.5267288957492976e-05, 'epoch': 0.28}\n",
      "{'loss': 0.5106, 'learning_rate': 4.514593739230048e-05, 'epoch': 0.29}\n",
      "{'loss': 0.4909, 'learning_rate': 4.5024585827108e-05, 'epoch': 0.3}\n",
      "{'loss': 0.4939, 'learning_rate': 4.490323426191551e-05, 'epoch': 0.31}\n",
      "{'loss': 0.49, 'learning_rate': 4.4781882696723024e-05, 'epoch': 0.31}\n",
      "{'loss': 0.5021, 'learning_rate': 4.466053113153054e-05, 'epoch': 0.32}\n",
      "{'loss': 0.5071, 'learning_rate': 4.4539179566338044e-05, 'epoch': 0.33}\n",
      "{'loss': 0.4891, 'learning_rate': 4.4417828001145565e-05, 'epoch': 0.33}\n",
      "{'loss': 0.5071, 'learning_rate': 4.429647643595307e-05, 'epoch': 0.34}\n",
      "{'loss': 0.4932, 'learning_rate': 4.4175124870760585e-05, 'epoch': 0.35}\n",
      " 12% 24319/206013 [7:05:21<53:17:06,  1.06s/it]"
     ]
    }
   ],
   "source": [
    "# Train the pretrained Electra Model on SNLI Data for Analysis (approach A0)\n",
    "!python run.py --do_train --task nli --dataset snli --output_dir ./trained_model/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ffAx5EBQrol",
    "outputId": "a93412a1-2c8a-4f30-f97d-a4466d7acb66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 05:32:14.562684: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 05:32:14.562753: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 05:32:14.562792: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 05:32:15.856058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100% 9842/9842 [00:02<00:00, 3718.15 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 1231/1231 [00:23<00:00, 53.12it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 0.3840707540512085, 'eval_accuracy': 0.8915870785713196, 'eval_runtime': 24.0075, 'eval_samples_per_second': 409.956, 'eval_steps_per_second': 51.276}\n"
     ]
    }
   ],
   "source": [
    "# do eval with old model on SNLI testset  (approach A0)\n",
    "!python3 run.py --do_eval --task nli --dataset snli --model ./trained_model/checkpoint-206000 --output_dir ./results_SNLI_old_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FX8zEA_2STJi",
    "outputId": "1b75b68d-276f-4b0f-e9e3-28ee2522c3c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-4063cc8330b9>:6: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  available_datasets = list_datasets()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if ANLI is available in HuggingFace datasets\n",
    "\n",
    "from datasets import list_datasets\n",
    "\n",
    "# Listing available datasets from the Hugging Face datasets library\n",
    "available_datasets = list_datasets()\n",
    "print('anli' in available_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQnXK_MKSGDF",
    "outputId": "48d99ff5-00ac-4dfb-d88e-321c5ed43749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-26 21:48:23.588220: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-26 21:48:23.588304: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-26 21:48:23.588409: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-26 21:48:25.789352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100% 1000/1000 [00:00<00:00, 1609.87 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 125/125 [00:55<00:00,  2.27it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 2.3898916244506836, 'eval_accuracy': 0.30300000309944153, 'eval_runtime': 55.4625, 'eval_samples_per_second': 18.03, 'eval_steps_per_second': 2.254}\n"
     ]
    }
   ],
   "source": [
    "# do eval with ANLI testsetr1 on base Electra trained on SNLI model (approach A0)\n",
    "!python3 run_ANLI.py --do_eval --task nli --dataset anli --model ./trained_model/checkpoint-206000 --output_dir ./results_ANLI_testr1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEsi6ck6py5D",
    "outputId": "99723e38-c5d2-49ab-829d-02328ea23081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100% 5.55k/5.55k [00:00<00:00, 23.4MB/s]\n",
      "Downloading metadata: 100% 2.76k/2.76k [00:00<00:00, 17.2MB/s]\n",
      "Downloading readme: 100% 7.47k/7.47k [00:00<00:00, 25.7MB/s]\n",
      "Downloading data: 100% 18.6M/18.6M [00:01<00:00, 12.0MB/s]\n",
      "Generating train_r1 split: 100% 16946/16946 [00:00<00:00, 19174.76 examples/s]\n",
      "Generating dev_r1 split: 100% 1000/1000 [00:00<00:00, 20489.50 examples/s]\n",
      "Generating test_r1 split: 100% 1000/1000 [00:00<00:00, 19388.36 examples/s]\n",
      "Generating train_r2 split: 100% 45460/45460 [00:02<00:00, 20789.63 examples/s]\n",
      "Generating dev_r2 split: 100% 1000/1000 [00:00<00:00, 21381.84 examples/s]\n",
      "Generating test_r2 split: 100% 1000/1000 [00:00<00:00, 19798.18 examples/s]\n",
      "Generating train_r3 split: 100% 100459/100459 [00:07<00:00, 12942.78 examples/s]\n",
      "Generating dev_r3 split: 100% 1200/1200 [00:00<00:00, 20740.18 examples/s]\n",
      "Generating test_r3 split: 100% 1200/1200 [00:00<00:00, 21876.76 examples/s]\n",
      "{'uid': '0fd0abfb-659e-4453-b196-c3a64d2d8267', 'premise': 'The Parma trolleybus system (Italian: \"Rete filoviaria di Parma\" ) forms part of the public transport network of the city and \"comune\" of Parma, in the region of Emilia-Romagna, northern Italy. In operation since 1953, the system presently comprises four urban routes.', 'hypothesis': 'The trolleybus system has over 2 urban routes', 'label': 0, 'reason': ''}\n",
      "{'uid': '7ed72ff4-40b7-4f8a-b1b9-6c612aa62c84', 'premise': 'Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series \"The Champions\". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate.', 'hypothesis': \"Sharron Macready was a popular character through the 1980's.\", 'label': 1, 'reason': ''}\n",
      "{'uid': '5d2930a3-62ac-485d-94d7-4e36cbbcd7b5', 'premise': 'Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series \"The Champions\". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate.', 'hypothesis': \"Bastedo didn't keep any pets because of her views on animal rights.\", 'label': 1, 'reason': ''}\n",
      "{'uid': '324db753-ddc9-4a85-a825-f09e2e5aebdd', 'premise': 'Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series \"The Champions\". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate.', 'hypothesis': 'Alexandra Bastedo was named by her mother.', 'label': 1, 'reason': ''}\n",
      "{'uid': '4874f429-da0e-406a-90c7-22240ff3ddf8', 'premise': 'Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series \"The Champions\". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate.', 'hypothesis': 'Bastedo cared for all the animals that inhabit the earth.', 'label': 1, 'reason': ''}\n",
      "162865\n",
      "16946\n",
      "45460\n",
      "100459\n",
      "2023-11-27 03:19:51.016301: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 03:19:51.016376: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 03:19:51.016425: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 03:19:52.231909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Filter: 100% 16946/16946 [00:00<00:00, 107274.54 examples/s]\n",
      "Filter: 100% 1000/1000 [00:00<00:00, 107900.39 examples/s]\n",
      "Filter: 100% 1000/1000 [00:00<00:00, 108863.79 examples/s]\n",
      "Filter: 100% 45460/45460 [00:00<00:00, 157514.91 examples/s]\n",
      "Filter: 100% 1000/1000 [00:00<00:00, 111119.16 examples/s]\n",
      "Filter: 100% 1000/1000 [00:00<00:00, 111943.63 examples/s]\n",
      "Filter: 100% 100459/100459 [00:00<00:00, 160317.44 examples/s]\n",
      "Filter: 100% 1200/1200 [00:00<00:00, 105894.48 examples/s]\n",
      "Filter: 100% 1200/1200 [00:00<00:00, 116236.69 examples/s]\n",
      "Map (num_proc=2): 100% 162865/162865 [01:02<00:00, 2622.59 examples/s]\n",
      "  0% 0/61077 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.8999, 'learning_rate': 4.959068061627127e-05, 'epoch': 0.02}\n",
      "{'loss': 0.8285, 'learning_rate': 4.918136123254253e-05, 'epoch': 0.05}\n",
      "{'loss': 0.7649, 'learning_rate': 4.8772041848813795e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7674, 'learning_rate': 4.836272246508506e-05, 'epoch': 0.1}\n",
      "{'loss': 0.7503, 'learning_rate': 4.7953403081356324e-05, 'epoch': 0.12}\n",
      "{'loss': 0.7456, 'learning_rate': 4.754408369762759e-05, 'epoch': 0.15}\n",
      "{'loss': 0.7306, 'learning_rate': 4.7134764313898846e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7269, 'learning_rate': 4.672544493017012e-05, 'epoch': 0.2}\n",
      "{'loss': 0.7116, 'learning_rate': 4.631612554644138e-05, 'epoch': 0.22}\n",
      "{'loss': 0.7076, 'learning_rate': 4.5906806162712646e-05, 'epoch': 0.25}\n",
      "{'loss': 0.6976, 'learning_rate': 4.5497486778983904e-05, 'epoch': 0.27}\n",
      "{'loss': 0.7248, 'learning_rate': 4.508816739525517e-05, 'epoch': 0.29}\n",
      "{'loss': 0.6837, 'learning_rate': 4.467884801152644e-05, 'epoch': 0.32}\n",
      "{'loss': 0.7022, 'learning_rate': 4.4269528627797703e-05, 'epoch': 0.34}\n",
      "{'loss': 0.7041, 'learning_rate': 4.386020924406896e-05, 'epoch': 0.37}\n",
      "{'loss': 0.6985, 'learning_rate': 4.3450889860340225e-05, 'epoch': 0.39}\n",
      "{'loss': 0.6775, 'learning_rate': 4.304157047661149e-05, 'epoch': 0.42}\n",
      "{'loss': 0.6696, 'learning_rate': 4.263225109288276e-05, 'epoch': 0.44}\n",
      "{'loss': 0.695, 'learning_rate': 4.2222931709154025e-05, 'epoch': 0.47}\n",
      "{'loss': 0.6662, 'learning_rate': 4.181361232542528e-05, 'epoch': 0.49}\n",
      "{'loss': 0.6682, 'learning_rate': 4.140429294169655e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6751, 'learning_rate': 4.099497355796781e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6666, 'learning_rate': 4.058565417423908e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6685, 'learning_rate': 4.017633479051034e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6562, 'learning_rate': 3.9767015406781605e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6775, 'learning_rate': 3.935769602305287e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6402, 'learning_rate': 3.8948376639324133e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6413, 'learning_rate': 3.85390572555954e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6644, 'learning_rate': 3.812973787186666e-05, 'epoch': 0.71}\n",
      "{'loss': 0.654, 'learning_rate': 3.7720418488137926e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6471, 'learning_rate': 3.731109910440919e-05, 'epoch': 0.76}\n",
      "{'loss': 0.651, 'learning_rate': 3.690177972068045e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6694, 'learning_rate': 3.649246033695172e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6385, 'learning_rate': 3.6083140953222984e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6736, 'learning_rate': 3.567382156949425e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6076, 'learning_rate': 3.526450218576551e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6177, 'learning_rate': 3.485518280203677e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6415, 'learning_rate': 3.444586341830804e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6437, 'learning_rate': 3.4036544034579306e-05, 'epoch': 0.96}\n",
      "{'loss': 0.6105, 'learning_rate': 3.362722465085057e-05, 'epoch': 0.98}\n",
      "{'loss': 0.6127, 'learning_rate': 3.321790526712183e-05, 'epoch': 1.01}\n",
      "{'loss': 0.5299, 'learning_rate': 3.280858588339309e-05, 'epoch': 1.03}\n",
      "{'loss': 0.5462, 'learning_rate': 3.239926649966436e-05, 'epoch': 1.06}\n",
      "{'loss': 0.5703, 'learning_rate': 3.198994711593563e-05, 'epoch': 1.08}\n",
      "{'loss': 0.5376, 'learning_rate': 3.1580627732206885e-05, 'epoch': 1.11}\n",
      "{'loss': 0.5451, 'learning_rate': 3.117130834847815e-05, 'epoch': 1.13}\n",
      "{'loss': 0.5522, 'learning_rate': 3.0761988964749414e-05, 'epoch': 1.15}\n",
      "{'loss': 0.5408, 'learning_rate': 3.035266958102068e-05, 'epoch': 1.18}\n",
      "{'loss': 0.5438, 'learning_rate': 2.9943350197291943e-05, 'epoch': 1.2}\n",
      "{'loss': 0.5394, 'learning_rate': 2.9534030813563207e-05, 'epoch': 1.23}\n",
      "{'loss': 0.5584, 'learning_rate': 2.912471142983447e-05, 'epoch': 1.25}\n",
      "{'loss': 0.5238, 'learning_rate': 2.871539204610574e-05, 'epoch': 1.28}\n",
      "{'loss': 0.5607, 'learning_rate': 2.8306072662377003e-05, 'epoch': 1.3}\n",
      "{'loss': 0.5106, 'learning_rate': 2.7896753278648264e-05, 'epoch': 1.33}\n",
      "{'loss': 0.5495, 'learning_rate': 2.748743389491953e-05, 'epoch': 1.35}\n",
      "{'loss': 0.5188, 'learning_rate': 2.7078114511190793e-05, 'epoch': 1.38}\n",
      "{'loss': 0.536, 'learning_rate': 2.666879512746206e-05, 'epoch': 1.4}\n",
      "{'loss': 0.5465, 'learning_rate': 2.625947574373332e-05, 'epoch': 1.42}\n",
      "{'loss': 0.5369, 'learning_rate': 2.5850156360004586e-05, 'epoch': 1.45}\n",
      "{'loss': 0.5509, 'learning_rate': 2.544083697627585e-05, 'epoch': 1.47}\n",
      "{'loss': 0.5338, 'learning_rate': 2.5031517592547115e-05, 'epoch': 1.5}\n",
      "{'loss': 0.5229, 'learning_rate': 2.462219820881838e-05, 'epoch': 1.52}\n",
      "{'loss': 0.5322, 'learning_rate': 2.421287882508964e-05, 'epoch': 1.55}\n",
      "{'loss': 0.5298, 'learning_rate': 2.3803559441360908e-05, 'epoch': 1.57}\n",
      "{'loss': 0.5255, 'learning_rate': 2.339424005763217e-05, 'epoch': 1.6}\n",
      "{'loss': 0.5361, 'learning_rate': 2.2984920673903433e-05, 'epoch': 1.62}\n",
      "{'loss': 0.5228, 'learning_rate': 2.25756012901747e-05, 'epoch': 1.65}\n",
      "{'loss': 0.5277, 'learning_rate': 2.2166281906445962e-05, 'epoch': 1.67}\n",
      "{'loss': 0.514, 'learning_rate': 2.1756962522717226e-05, 'epoch': 1.69}\n",
      "{'loss': 0.5343, 'learning_rate': 2.134764313898849e-05, 'epoch': 1.72}\n",
      "{'loss': 0.5259, 'learning_rate': 2.0938323755259755e-05, 'epoch': 1.74}\n",
      "{'loss': 0.5524, 'learning_rate': 2.052900437153102e-05, 'epoch': 1.77}\n",
      "{'loss': 0.5244, 'learning_rate': 2.0119684987802284e-05, 'epoch': 1.79}\n",
      "{'loss': 0.5253, 'learning_rate': 1.9710365604073548e-05, 'epoch': 1.82}\n",
      "{'loss': 0.5221, 'learning_rate': 1.9301046220344813e-05, 'epoch': 1.84}\n",
      "{'loss': 0.5353, 'learning_rate': 1.8891726836616074e-05, 'epoch': 1.87}\n",
      "{'loss': 0.4952, 'learning_rate': 1.848240745288734e-05, 'epoch': 1.89}\n",
      "{'loss': 0.5359, 'learning_rate': 1.8073088069158602e-05, 'epoch': 1.92}\n",
      "{'loss': 0.5223, 'learning_rate': 1.766376868542987e-05, 'epoch': 1.94}\n",
      "{'loss': 0.5159, 'learning_rate': 1.725444930170113e-05, 'epoch': 1.96}\n",
      "{'loss': 0.5088, 'learning_rate': 1.6845129917972395e-05, 'epoch': 1.99}\n",
      "{'loss': 0.4738, 'learning_rate': 1.643581053424366e-05, 'epoch': 2.01}\n",
      "{'loss': 0.4264, 'learning_rate': 1.6026491150514924e-05, 'epoch': 2.04}\n",
      "{'loss': 0.4364, 'learning_rate': 1.561717176678619e-05, 'epoch': 2.06}\n",
      "{'loss': 0.4508, 'learning_rate': 1.5207852383057453e-05, 'epoch': 2.09}\n",
      "{'loss': 0.4172, 'learning_rate': 1.4798532999328719e-05, 'epoch': 2.11}\n",
      "{'loss': 0.44, 'learning_rate': 1.438921361559998e-05, 'epoch': 2.14}\n",
      "{'loss': 0.442, 'learning_rate': 1.3979894231871246e-05, 'epoch': 2.16}\n",
      "{'loss': 0.4381, 'learning_rate': 1.3570574848142509e-05, 'epoch': 2.19}\n",
      "{'loss': 0.4364, 'learning_rate': 1.3161255464413775e-05, 'epoch': 2.21}\n",
      "{'loss': 0.4283, 'learning_rate': 1.2751936080685037e-05, 'epoch': 2.23}\n",
      "{'loss': 0.4568, 'learning_rate': 1.2342616696956302e-05, 'epoch': 2.26}\n",
      "{'loss': 0.4202, 'learning_rate': 1.1933297313227566e-05, 'epoch': 2.28}\n",
      "{'loss': 0.4357, 'learning_rate': 1.152397792949883e-05, 'epoch': 2.31}\n",
      "{'loss': 0.4103, 'learning_rate': 1.1114658545770095e-05, 'epoch': 2.33}\n",
      "{'loss': 0.4394, 'learning_rate': 1.0705339162041359e-05, 'epoch': 2.36}\n",
      "{'loss': 0.427, 'learning_rate': 1.0296019778312622e-05, 'epoch': 2.38}\n",
      "{'loss': 0.4213, 'learning_rate': 9.886700394583886e-06, 'epoch': 2.41}\n",
      "{'loss': 0.4517, 'learning_rate': 9.47738101085515e-06, 'epoch': 2.43}\n",
      "{'loss': 0.4358, 'learning_rate': 9.068061627126415e-06, 'epoch': 2.46}\n",
      "{'loss': 0.4387, 'learning_rate': 8.65874224339768e-06, 'epoch': 2.48}\n",
      "{'loss': 0.4257, 'learning_rate': 8.249422859668942e-06, 'epoch': 2.51}\n",
      "{'loss': 0.4541, 'learning_rate': 7.840103475940206e-06, 'epoch': 2.53}\n",
      "{'loss': 0.4449, 'learning_rate': 7.430784092211471e-06, 'epoch': 2.55}\n",
      "{'loss': 0.4327, 'learning_rate': 7.021464708482735e-06, 'epoch': 2.58}\n",
      "{'loss': 0.4278, 'learning_rate': 6.6121453247539985e-06, 'epoch': 2.6}\n",
      "{'loss': 0.4246, 'learning_rate': 6.202825941025264e-06, 'epoch': 2.63}\n",
      "{'loss': 0.4609, 'learning_rate': 5.793506557296527e-06, 'epoch': 2.65}\n",
      "{'loss': 0.4205, 'learning_rate': 5.384187173567792e-06, 'epoch': 2.68}\n",
      "{'loss': 0.4097, 'learning_rate': 4.974867789839056e-06, 'epoch': 2.7}\n",
      "{'loss': 0.4309, 'learning_rate': 4.5655484061103195e-06, 'epoch': 2.73}\n",
      "{'loss': 0.4438, 'learning_rate': 4.156229022381584e-06, 'epoch': 2.75}\n",
      "{'loss': 0.4115, 'learning_rate': 3.7469096386528486e-06, 'epoch': 2.78}\n",
      "{'loss': 0.431, 'learning_rate': 3.3375902549241126e-06, 'epoch': 2.8}\n",
      "{'loss': 0.4279, 'learning_rate': 2.9282708711953765e-06, 'epoch': 2.82}\n",
      "{'loss': 0.4279, 'learning_rate': 2.5189514874666404e-06, 'epoch': 2.85}\n",
      "{'loss': 0.4032, 'learning_rate': 2.109632103737905e-06, 'epoch': 2.87}\n",
      "{'loss': 0.4171, 'learning_rate': 1.7003127200091687e-06, 'epoch': 2.9}\n",
      "{'loss': 0.434, 'learning_rate': 1.2909933362804329e-06, 'epoch': 2.92}\n",
      "{'loss': 0.4094, 'learning_rate': 8.816739525516971e-07, 'epoch': 2.95}\n",
      "{'loss': 0.3917, 'learning_rate': 4.7235456882296124e-07, 'epoch': 2.97}\n",
      "{'loss': 0.4191, 'learning_rate': 6.303518509422533e-08, 'epoch': 3.0}\n",
      "{'train_runtime': 3339.759, 'train_samples_per_second': 146.296, 'train_steps_per_second': 18.288, 'train_loss': 0.5507024133603774, 'epoch': 3.0}\n",
      "100% 61077/61077 [55:39<00:00, 18.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train existing SNLI trained ELECTRA model on combined ANLI train data (Approach A1)\n",
    "!python3 run_ANLI_train.py --do_train --task nli --dataset anli --model ./trained_model/checkpoint-206000 --output_dir ./results_ANLI_train/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Ve_RTYZqTXs",
    "outputId": "6b5a3f86-1aad-4a8a-a1c1-25e6a2abe5d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 06:13:46.618744: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-04 06:13:46.618830: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-04 06:13:46.618919: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-04 06:13:47.802972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100% 1000/1000 [00:00<00:00, 2467.33 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 125/125 [00:38<00:00,  3.22it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 1.5161266326904297, 'eval_accuracy': 0.5139999985694885, 'eval_runtime': 39.1765, 'eval_samples_per_second': 25.525, 'eval_steps_per_second': 3.191}\n"
     ]
    }
   ],
   "source": [
    "# do eval with SNLI model trained on ANLI testset1 (Approach A1)\n",
    "!python3 run_ANLI.py --do_eval --task nli --dataset anli --model ./results_ANLI_train/checkpoint-59500 --output_dir ./results_ANLI_new_testr1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDkLJXL0SjNU",
    "outputId": "f755d5fa-8f69-4bfe-fa1a-49f3d0d3789c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 06:17:04.514927: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-04 06:17:04.514986: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-04 06:17:04.515025: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-04 06:17:05.941169: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 125/125 [00:38<00:00,  3.28it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 1.9780179262161255, 'eval_accuracy': 0.4099999964237213, 'eval_runtime': 38.3999, 'eval_samples_per_second': 26.042, 'eval_steps_per_second': 3.255}\n"
     ]
    }
   ],
   "source": [
    "# do eval with SNLI model trained on ANLI testset2 (Approach A1)\n",
    "!python3 run_ANLI.py --do_eval --task nli --dataset anli --model ./results_ANLI_train/checkpoint-59500 --output_dir ./results_ANLI_new_testr3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UW8R17rgSnBE",
    "outputId": "245217e0-a782-418c-cf01-90819c3cdd75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 06:18:28.291349: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-04 06:18:28.291397: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-04 06:18:28.291436: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-04 06:18:29.424822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100% 1200/1200 [00:00<00:00, 3002.18 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 150/150 [00:46<00:00,  3.26it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 1.9766002893447876, 'eval_accuracy': 0.4375, 'eval_runtime': 46.3422, 'eval_samples_per_second': 25.894, 'eval_steps_per_second': 3.237}\n"
     ]
    }
   ],
   "source": [
    "# do eval with new model on ANLI testset3 (Approach A1)\n",
    "!python3 run_ANLI.py --do_eval --task nli --dataset anli --model ./results_ANLI_train/checkpoint-59500 --output_dir ./results_ANLI_new_testr3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ivjCSX_P21B",
    "outputId": "515fcf5d-b800-448e-ce70-5ea184be513e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 05:30:00.511749: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 05:30:00.511812: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 05:30:00.511852: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 05:30:01.876951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading builder script: 100% 3.82k/3.82k [00:00<00:00, 19.6MB/s]\n",
      "Downloading metadata: 100% 1.90k/1.90k [00:00<00:00, 10.6MB/s]\n",
      "Downloading readme: 100% 14.1k/14.1k [00:00<00:00, 31.1MB/s]\n",
      "Downloading: 100% 1.93k/1.93k [00:00<00:00, 9.14MB/s]\n",
      "Downloading: 100% 1.26M/1.26M [00:00<00:00, 3.00MB/s]\n",
      "Downloading: 100% 65.9M/65.9M [00:02<00:00, 30.7MB/s]\n",
      "Downloading: 100% 1.26M/1.26M [00:00<00:00, 3.05MB/s]\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Filter: 100% 10000/10000 [00:00<00:00, 200150.99 examples/s]\n",
      "Filter: 100% 550152/550152 [00:02<00:00, 263938.47 examples/s]\n",
      "Filter: 100% 10000/10000 [00:00<00:00, 261480.49 examples/s]\n",
      "Map (num_proc=2): 100% 9842/9842 [00:01<00:00, 5843.13 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 1231/1231 [00:24<00:00, 49.88it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 0.6582831740379333, 'eval_accuracy': 0.815586268901825, 'eval_runtime': 25.2674, 'eval_samples_per_second': 389.514, 'eval_steps_per_second': 48.719}\n"
     ]
    }
   ],
   "source": [
    "# do eval with new model on SNLI testset (Approach A1)\n",
    "!python3 run.py --do_eval --task nli --dataset snli --model ./results_ANLI_train/checkpoint-56000 --output_dir ./results_SNLI_new_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oK9bSYblgOd5",
    "outputId": "730db1dd-c9b5-4e03-a3b4-8cabbcb195cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 06:45:20.622963: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 06:45:20.623025: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 06:45:20.623074: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 06:45:22.336025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading builder script: 100% 3.82k/3.82k [00:00<00:00, 22.1MB/s]\n",
      "Downloading metadata: 100% 1.90k/1.90k [00:00<00:00, 11.1MB/s]\n",
      "Downloading readme: 100% 14.1k/14.1k [00:00<00:00, 45.5MB/s]\n",
      "Downloading: 100% 1.93k/1.93k [00:00<00:00, 10.6MB/s]\n",
      "Downloading: 100% 1.26M/1.26M [00:00<00:00, 8.77MB/s]\n",
      "Downloading: 100% 65.9M/65.9M [00:00<00:00, 66.4MB/s]\n",
      "Downloading: 100% 1.26M/1.26M [00:00<00:00, 9.22MB/s]\n",
      "Downloading builder script: 100% 5.55k/5.55k [00:00<00:00, 23.1MB/s]\n",
      "Downloading metadata: 100% 2.76k/2.76k [00:00<00:00, 14.9MB/s]\n",
      "Downloading readme: 100% 7.47k/7.47k [00:00<00:00, 27.5MB/s]\n",
      "Downloading data: 100% 18.6M/18.6M [00:00<00:00, 73.1MB/s]\n",
      "Generating train_r1 split: 100% 16946/16946 [00:01<00:00, 16413.82 examples/s]\n",
      "Generating dev_r1 split: 100% 1000/1000 [00:00<00:00, 15594.35 examples/s]\n",
      "Generating test_r1 split: 100% 1000/1000 [00:00<00:00, 17137.24 examples/s]\n",
      "Generating train_r2 split: 100% 45460/45460 [00:03<00:00, 11738.32 examples/s]\n",
      "Generating dev_r2 split: 100% 1000/1000 [00:00<00:00, 8689.54 examples/s]\n",
      "Generating test_r2 split: 100% 1000/1000 [00:00<00:00, 7836.79 examples/s]\n",
      "Generating train_r3 split: 100% 100459/100459 [00:07<00:00, 12660.00 examples/s]\n",
      "Generating dev_r3 split: 100% 1200/1200 [00:00<00:00, 16849.61 examples/s]\n",
      "Generating test_r3 split: 100% 1200/1200 [00:00<00:00, 15369.62 examples/s]\n",
      "config.json: 100% 665/665 [00:00<00:00, 3.85MB/s]\n",
      "pytorch_model.bin: 100% 54.2M/54.2M [00:00<00:00, 169MB/s]\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "tokenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 163kB/s]\n",
      "vocab.txt: 100% 232k/232k [00:00<00:00, 5.59MB/s]\n",
      "tokenizer.json: 100% 466k/466k [00:00<00:00, 6.10MB/s]\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Filter: 100% 10000/10000 [00:00<00:00, 102130.71 examples/s]\n",
      "Filter: 100% 550152/550152 [00:02<00:00, 250472.56 examples/s]\n",
      "Filter: 100% 10000/10000 [00:00<00:00, 244212.68 examples/s]\n",
      "Filter: 100% 16946/16946 [00:00<00:00, 151165.86 examples/s]\n",
      "Filter: 100% 1000/1000 [00:00<00:00, 113060.11 examples/s]\n",
      "Filter: 100% 1000/1000 [00:00<00:00, 118156.07 examples/s]\n",
      "Filter: 100% 45460/45460 [00:00<00:00, 163527.64 examples/s]\n",
      "Filter: 100% 1000/1000 [00:00<00:00, 119847.53 examples/s]\n",
      "Filter: 100% 1000/1000 [00:00<00:00, 118556.84 examples/s]\n",
      "Filter: 100% 100459/100459 [00:00<00:00, 159142.21 examples/s]\n",
      "Filter: 100% 1200/1200 [00:00<00:00, 115426.32 examples/s]\n",
      "Filter: 100% 1200/1200 [00:00<00:00, 118813.20 examples/s]\n",
      "len traindata merged: 712232\n",
      "Map (num_proc=2): 100% 712232/712232 [02:54<00:00, 4079.81 examples/s]\n",
      "  0% 0/267087 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.9893, 'learning_rate': 4.99063975408762e-05, 'epoch': 0.01}\n",
      "{'loss': 0.8225, 'learning_rate': 4.9812795081752386e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7652, 'learning_rate': 4.9719192622628584e-05, 'epoch': 0.02}\n",
      "{'loss': 0.7448, 'learning_rate': 4.9625590163504776e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6997, 'learning_rate': 4.9531987704380975e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6704, 'learning_rate': 4.9438385245257167e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6604, 'learning_rate': 4.934478278613336e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6622, 'learning_rate': 4.925118032700956e-05, 'epoch': 0.04}\n",
      "{'loss': 0.657, 'learning_rate': 4.915757786788575e-05, 'epoch': 0.05}\n",
      "{'loss': 0.6448, 'learning_rate': 4.906397540876194e-05, 'epoch': 0.06}\n",
      "{'loss': 0.6392, 'learning_rate': 4.897037294963813e-05, 'epoch': 0.06}\n",
      "{'loss': 0.6444, 'learning_rate': 4.887677049051433e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6345, 'learning_rate': 4.878316803139052e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6181, 'learning_rate': 4.8689565572266714e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6067, 'learning_rate': 4.859596311314291e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6364, 'learning_rate': 4.8502360654019105e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6139, 'learning_rate': 4.8408758194895296e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6186, 'learning_rate': 4.8315155735771495e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6318, 'learning_rate': 4.822155327664769e-05, 'epoch': 0.11}\n",
      "{'loss': 0.598, 'learning_rate': 4.812795081752388e-05, 'epoch': 0.11}\n",
      "{'loss': 0.6061, 'learning_rate': 4.803434835840007e-05, 'epoch': 0.12}\n",
      "{'loss': 0.6163, 'learning_rate': 4.794074589927627e-05, 'epoch': 0.12}\n",
      "{'loss': 0.6121, 'learning_rate': 4.784714344015247e-05, 'epoch': 0.13}\n",
      "{'loss': 0.6033, 'learning_rate': 4.775354098102865e-05, 'epoch': 0.13}\n",
      "{'loss': 0.6071, 'learning_rate': 4.765993852190485e-05, 'epoch': 0.14}\n",
      "{'loss': 0.5768, 'learning_rate': 4.756633606278104e-05, 'epoch': 0.15}\n",
      "{'loss': 0.5853, 'learning_rate': 4.747273360365724e-05, 'epoch': 0.15}\n",
      "{'loss': 0.5963, 'learning_rate': 4.737913114453343e-05, 'epoch': 0.16}\n",
      "{'loss': 0.5724, 'learning_rate': 4.7285528685409625e-05, 'epoch': 0.16}\n",
      "{'loss': 0.584, 'learning_rate': 4.719192622628582e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5939, 'learning_rate': 4.709832376716201e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5919, 'learning_rate': 4.700472130803821e-05, 'epoch': 0.18}\n",
      "{'loss': 0.5724, 'learning_rate': 4.69111188489144e-05, 'epoch': 0.19}\n",
      "{'loss': 0.5718, 'learning_rate': 4.68175163897906e-05, 'epoch': 0.19}\n",
      "{'loss': 0.5546, 'learning_rate': 4.672391393066679e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5738, 'learning_rate': 4.663031147154298e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5888, 'learning_rate': 4.653670901241918e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5826, 'learning_rate': 4.644310655329537e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5874, 'learning_rate': 4.634950409417156e-05, 'epoch': 0.22}\n",
      "{'loss': 0.5894, 'learning_rate': 4.625590163504776e-05, 'epoch': 0.22}\n",
      "{'loss': 0.5512, 'learning_rate': 4.616229917592395e-05, 'epoch': 0.23}\n",
      "{'loss': 0.5628, 'learning_rate': 4.6068696716800145e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5731, 'learning_rate': 4.597509425767634e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5638, 'learning_rate': 4.5881491798552535e-05, 'epoch': 0.25}\n",
      "{'loss': 0.5552, 'learning_rate': 4.578788933942873e-05, 'epoch': 0.25}\n",
      "{'loss': 0.536, 'learning_rate': 4.569428688030492e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5461, 'learning_rate': 4.560068442118112e-05, 'epoch': 0.26}\n",
      "{'loss': 0.542, 'learning_rate': 4.550708196205731e-05, 'epoch': 0.27}\n",
      "{'loss': 0.5523, 'learning_rate': 4.54134795029335e-05, 'epoch': 0.28}\n",
      "{'loss': 0.5533, 'learning_rate': 4.53198770438097e-05, 'epoch': 0.28}\n",
      "{'loss': 0.5606, 'learning_rate': 4.522627458468589e-05, 'epoch': 0.29}\n",
      "{'loss': 0.5715, 'learning_rate': 4.513267212556209e-05, 'epoch': 0.29}\n",
      "{'loss': 0.5345, 'learning_rate': 4.5039069666438275e-05, 'epoch': 0.3}\n",
      "{'loss': 0.5496, 'learning_rate': 4.494546720731447e-05, 'epoch': 0.3}\n",
      "{'loss': 0.54, 'learning_rate': 4.4851864748190665e-05, 'epoch': 0.31}\n",
      "{'loss': 0.5413, 'learning_rate': 4.475826228906686e-05, 'epoch': 0.31}\n",
      "{'loss': 0.555, 'learning_rate': 4.4664659829943055e-05, 'epoch': 0.32}\n",
      "{'loss': 0.5279, 'learning_rate': 4.457105737081925e-05, 'epoch': 0.33}\n",
      "{'loss': 0.5336, 'learning_rate': 4.4477454911695446e-05, 'epoch': 0.33}\n",
      "{'loss': 0.5289, 'learning_rate': 4.438385245257163e-05, 'epoch': 0.34}\n",
      "{'loss': 0.5506, 'learning_rate': 4.429024999344783e-05, 'epoch': 0.34}\n",
      "{'loss': 0.5418, 'learning_rate': 4.419664753432403e-05, 'epoch': 0.35}\n",
      "{'loss': 0.5368, 'learning_rate': 4.410304507520022e-05, 'epoch': 0.35}\n",
      "{'loss': 0.5361, 'learning_rate': 4.400944261607641e-05, 'epoch': 0.36}\n",
      "{'loss': 0.5473, 'learning_rate': 4.39158401569526e-05, 'epoch': 0.37}\n",
      "{'loss': 0.536, 'learning_rate': 4.38222376978288e-05, 'epoch': 0.37}\n",
      "{'loss': 0.5374, 'learning_rate': 4.372863523870499e-05, 'epoch': 0.38}\n",
      "{'loss': 0.5479, 'learning_rate': 4.3635032779581185e-05, 'epoch': 0.38}\n",
      "{'loss': 0.5339, 'learning_rate': 4.3541430320457384e-05, 'epoch': 0.39}\n",
      "{'loss': 0.5389, 'learning_rate': 4.3447827861333575e-05, 'epoch': 0.39}\n",
      "{'loss': 0.5423, 'learning_rate': 4.335422540220977e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5309, 'learning_rate': 4.3260622943085966e-05, 'epoch': 0.4}\n",
      "{'loss': 0.534, 'learning_rate': 4.316702048396216e-05, 'epoch': 0.41}\n",
      "{'loss': 0.5456, 'learning_rate': 4.307341802483835e-05, 'epoch': 0.42}\n",
      "{'loss': 0.5159, 'learning_rate': 4.297981556571454e-05, 'epoch': 0.42}\n",
      "{'loss': 0.5137, 'learning_rate': 4.288621310659074e-05, 'epoch': 0.43}\n",
      "{'loss': 0.5252, 'learning_rate': 4.279261064746693e-05, 'epoch': 0.43}\n",
      "{'loss': 0.5179, 'learning_rate': 4.269900818834312e-05, 'epoch': 0.44}\n",
      "{'loss': 0.5322, 'learning_rate': 4.260540572921932e-05, 'epoch': 0.44}\n",
      "{'loss': 0.5247, 'learning_rate': 4.2511803270095513e-05, 'epoch': 0.45}\n",
      "{'loss': 0.5502, 'learning_rate': 4.241820081097171e-05, 'epoch': 0.45}\n",
      "{'loss': 0.5256, 'learning_rate': 4.23245983518479e-05, 'epoch': 0.46}\n",
      "{'loss': 0.529, 'learning_rate': 4.2230995892724096e-05, 'epoch': 0.47}\n",
      "{'loss': 0.5395, 'learning_rate': 4.2137393433600294e-05, 'epoch': 0.47}\n",
      "{'loss': 0.5284, 'learning_rate': 4.204379097447648e-05, 'epoch': 0.48}\n",
      "{'loss': 0.5408, 'learning_rate': 4.195018851535268e-05, 'epoch': 0.48}\n",
      "{'loss': 0.5185, 'learning_rate': 4.185658605622887e-05, 'epoch': 0.49}\n",
      "{'loss': 0.5296, 'learning_rate': 4.176298359710507e-05, 'epoch': 0.49}\n",
      "{'loss': 0.5472, 'learning_rate': 4.166938113798126e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5034, 'learning_rate': 4.157577867885745e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5107, 'learning_rate': 4.148217621973365e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5196, 'learning_rate': 4.138857376060984e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5222, 'learning_rate': 4.1294971301486034e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5251, 'learning_rate': 4.120136884236223e-05, 'epoch': 0.53}\n",
      "{'loss': 0.502, 'learning_rate': 4.1107766383238424e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5152, 'learning_rate': 4.1014163924114616e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5074, 'learning_rate': 4.092056146499081e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5188, 'learning_rate': 4.0826959005867006e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5283, 'learning_rate': 4.07333565467432e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5398, 'learning_rate': 4.063975408761939e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5188, 'learning_rate': 4.054615162849559e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5061, 'learning_rate': 4.045254916937178e-05, 'epoch': 0.57}\n",
      "{'loss': 0.507, 'learning_rate': 4.035894671024797e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5332, 'learning_rate': 4.0265344251124163e-05, 'epoch': 0.58}\n",
      "{'loss': 0.4993, 'learning_rate': 4.017174179200036e-05, 'epoch': 0.59}\n",
      "{'loss': 0.4993, 'learning_rate': 4.007813933287656e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5052, 'learning_rate': 3.9984536873752745e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5018, 'learning_rate': 3.9890934414628944e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5156, 'learning_rate': 3.9797331955505136e-05, 'epoch': 0.61}\n",
      "{'loss': 0.53, 'learning_rate': 3.9703729496381334e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5246, 'learning_rate': 3.9610127037257526e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5039, 'learning_rate': 3.951652457813372e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5064, 'learning_rate': 3.9422922119009916e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5019, 'learning_rate': 3.93293196598861e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5063, 'learning_rate': 3.92357172007623e-05, 'epoch': 0.65}\n",
      "{'loss': 0.512, 'learning_rate': 3.914211474163849e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5072, 'learning_rate': 3.904851228251469e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5123, 'learning_rate': 3.895490982339088e-05, 'epoch': 0.66}\n",
      "{'loss': 0.51, 'learning_rate': 3.8861307364267074e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5129, 'learning_rate': 3.876770490514327e-05, 'epoch': 0.67}\n",
      "{'loss': 0.4946, 'learning_rate': 3.8674102446019464e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5212, 'learning_rate': 3.8580499986895656e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5036, 'learning_rate': 3.8486897527771854e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4918, 'learning_rate': 3.8393295068648046e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5245, 'learning_rate': 3.829969260952424e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5202, 'learning_rate': 3.820609015040043e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5039, 'learning_rate': 3.811248769127663e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5265, 'learning_rate': 3.801888523215282e-05, 'epoch': 0.72}\n",
      "{'loss': 0.5084, 'learning_rate': 3.792528277302901e-05, 'epoch': 0.72}\n",
      "{'loss': 0.4934, 'learning_rate': 3.783168031390521e-05, 'epoch': 0.73}\n",
      "{'loss': 0.4952, 'learning_rate': 3.77380778547814e-05, 'epoch': 0.74}\n",
      "{'loss': 0.4946, 'learning_rate': 3.7644475395657594e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5099, 'learning_rate': 3.755087293653379e-05, 'epoch': 0.75}\n",
      "{'loss': 0.5016, 'learning_rate': 3.7457270477409984e-05, 'epoch': 0.75}\n",
      "{'loss': 0.4918, 'learning_rate': 3.736366801828618e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5161, 'learning_rate': 3.727006555916237e-05, 'epoch': 0.76}\n",
      "{'loss': 0.4992, 'learning_rate': 3.7176463100038566e-05, 'epoch': 0.77}\n",
      "{'loss': 0.506, 'learning_rate': 3.708286064091476e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4988, 'learning_rate': 3.698925818179096e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4984, 'learning_rate': 3.689565572266715e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5108, 'learning_rate': 3.680205326354334e-05, 'epoch': 0.79}\n",
      "{'loss': 0.4958, 'learning_rate': 3.670845080441954e-05, 'epoch': 0.8}\n",
      "{'loss': 0.4923, 'learning_rate': 3.6614848345295724e-05, 'epoch': 0.8}\n",
      "{'loss': 0.4956, 'learning_rate': 3.652124588617192e-05, 'epoch': 0.81}\n",
      "{'loss': 0.4983, 'learning_rate': 3.642764342704812e-05, 'epoch': 0.81}\n",
      "{'loss': 0.493, 'learning_rate': 3.633404096792431e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5004, 'learning_rate': 3.6240438508800504e-05, 'epoch': 0.83}\n",
      "{'loss': 0.5071, 'learning_rate': 3.6146836049676696e-05, 'epoch': 0.83}\n",
      "{'loss': 0.4987, 'learning_rate': 3.6053233590552895e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5075, 'learning_rate': 3.5959631131429087e-05, 'epoch': 0.84}\n",
      "{'loss': 0.4954, 'learning_rate': 3.586602867230528e-05, 'epoch': 0.85}\n",
      "{'loss': 0.4942, 'learning_rate': 3.577242621318148e-05, 'epoch': 0.85}\n",
      "{'loss': 0.4966, 'learning_rate': 3.567882375405767e-05, 'epoch': 0.86}\n",
      "{'loss': 0.4936, 'learning_rate': 3.558522129493386e-05, 'epoch': 0.86}\n",
      "{'loss': 0.4746, 'learning_rate': 3.549161883581006e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5167, 'learning_rate': 3.539801637668625e-05, 'epoch': 0.88}\n",
      "{'loss': 0.4787, 'learning_rate': 3.530441391756244e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5014, 'learning_rate': 3.5210811458438634e-05, 'epoch': 0.89}\n",
      "{'loss': 0.4999, 'learning_rate': 3.511720899931483e-05, 'epoch': 0.89}\n",
      "{'loss': 0.4807, 'learning_rate': 3.5023606540191025e-05, 'epoch': 0.9}\n",
      "{'loss': 0.4871, 'learning_rate': 3.4930004081067216e-05, 'epoch': 0.9}\n",
      "{'loss': 0.4773, 'learning_rate': 3.4836401621943415e-05, 'epoch': 0.91}\n",
      "{'loss': 0.4824, 'learning_rate': 3.474279916281961e-05, 'epoch': 0.92}\n",
      "{'loss': 0.4919, 'learning_rate': 3.4649196703695805e-05, 'epoch': 0.92}\n",
      "{'loss': 0.504, 'learning_rate': 3.455559424457199e-05, 'epoch': 0.93}\n",
      "{'loss': 0.4756, 'learning_rate': 3.446199178544819e-05, 'epoch': 0.93}\n",
      "{'loss': 0.4754, 'learning_rate': 3.436838932632439e-05, 'epoch': 0.94}\n",
      "{'loss': 0.4956, 'learning_rate': 3.427478686720057e-05, 'epoch': 0.94}\n",
      "{'loss': 0.508, 'learning_rate': 3.418118440807677e-05, 'epoch': 0.95}\n",
      "{'loss': 0.484, 'learning_rate': 3.408758194895296e-05, 'epoch': 0.95}\n",
      "{'loss': 0.4944, 'learning_rate': 3.399397948982916e-05, 'epoch': 0.96}\n",
      "{'loss': 0.4841, 'learning_rate': 3.390037703070535e-05, 'epoch': 0.97}\n",
      "{'loss': 0.481, 'learning_rate': 3.3806774571581545e-05, 'epoch': 0.97}\n",
      "{'loss': 0.5041, 'learning_rate': 3.371317211245774e-05, 'epoch': 0.98}\n",
      "{'loss': 0.4823, 'learning_rate': 3.3619569653333935e-05, 'epoch': 0.98}\n",
      "{'loss': 0.4621, 'learning_rate': 3.352596719421013e-05, 'epoch': 0.99}\n",
      "{'loss': 0.4828, 'learning_rate': 3.3432364735086325e-05, 'epoch': 0.99}\n",
      "{'loss': 0.4975, 'learning_rate': 3.333876227596252e-05, 'epoch': 1.0}\n",
      "{'loss': 0.4571, 'learning_rate': 3.324515981683871e-05, 'epoch': 1.01}\n",
      "{'loss': 0.4417, 'learning_rate': 3.31515573577149e-05, 'epoch': 1.01}\n",
      "{'loss': 0.4569, 'learning_rate': 3.30579548985911e-05, 'epoch': 1.02}\n",
      "{'loss': 0.4437, 'learning_rate': 3.296435243946729e-05, 'epoch': 1.02}\n",
      "{'loss': 0.4475, 'learning_rate': 3.287074998034348e-05, 'epoch': 1.03}\n",
      "{'loss': 0.4423, 'learning_rate': 3.277714752121968e-05, 'epoch': 1.03}\n",
      "{'loss': 0.4723, 'learning_rate': 3.268354506209587e-05, 'epoch': 1.04}\n",
      "{'loss': 0.4281, 'learning_rate': 3.2589942602972065e-05, 'epoch': 1.04}\n",
      "{'loss': 0.4333, 'learning_rate': 3.2496340143848257e-05, 'epoch': 1.05}\n",
      "{'loss': 0.4588, 'learning_rate': 3.2402737684724455e-05, 'epoch': 1.06}\n",
      "{'loss': 0.4397, 'learning_rate': 3.2309135225600654e-05, 'epoch': 1.06}\n",
      "{'loss': 0.4451, 'learning_rate': 3.221553276647684e-05, 'epoch': 1.07}\n",
      "{'loss': 0.4626, 'learning_rate': 3.212193030735304e-05, 'epoch': 1.07}\n",
      "{'loss': 0.4469, 'learning_rate': 3.202832784822923e-05, 'epoch': 1.08}\n",
      "{'loss': 0.4593, 'learning_rate': 3.193472538910543e-05, 'epoch': 1.08}\n",
      "{'loss': 0.4561, 'learning_rate': 3.184112292998162e-05, 'epoch': 1.09}\n",
      "{'loss': 0.445, 'learning_rate': 3.174752047085781e-05, 'epoch': 1.1}\n",
      "{'loss': 0.4508, 'learning_rate': 3.165391801173401e-05, 'epoch': 1.1}\n",
      "{'loss': 0.4623, 'learning_rate': 3.1560315552610195e-05, 'epoch': 1.11}\n",
      "{'loss': 0.4225, 'learning_rate': 3.146671309348639e-05, 'epoch': 1.11}\n",
      "{'loss': 0.4518, 'learning_rate': 3.137311063436259e-05, 'epoch': 1.12}\n",
      "{'loss': 0.4555, 'learning_rate': 3.1279508175238783e-05, 'epoch': 1.12}\n",
      "{'loss': 0.4394, 'learning_rate': 3.1185905716114975e-05, 'epoch': 1.13}\n",
      "{'loss': 0.45, 'learning_rate': 3.109230325699117e-05, 'epoch': 1.13}\n",
      "{'loss': 0.4439, 'learning_rate': 3.0998700797867366e-05, 'epoch': 1.14}\n",
      "{'loss': 0.4597, 'learning_rate': 3.090509833874356e-05, 'epoch': 1.15}\n",
      "{'loss': 0.4449, 'learning_rate': 3.081149587961975e-05, 'epoch': 1.15}\n",
      "{'loss': 0.4563, 'learning_rate': 3.071789342049595e-05, 'epoch': 1.16}\n",
      "{'loss': 0.4674, 'learning_rate': 3.062429096137214e-05, 'epoch': 1.16}\n",
      "{'loss': 0.4676, 'learning_rate': 3.053068850224833e-05, 'epoch': 1.17}\n",
      "{'loss': 0.4704, 'learning_rate': 3.0437086043124523e-05, 'epoch': 1.17}\n",
      "{'loss': 0.432, 'learning_rate': 3.034348358400072e-05, 'epoch': 1.18}\n",
      "{'loss': 0.4334, 'learning_rate': 3.0249881124876917e-05, 'epoch': 1.19}\n",
      "{'loss': 0.4486, 'learning_rate': 3.015627866575311e-05, 'epoch': 1.19}\n",
      "{'loss': 0.439, 'learning_rate': 3.0062676206629304e-05, 'epoch': 1.2}\n",
      "{'loss': 0.4279, 'learning_rate': 2.9969073747505495e-05, 'epoch': 1.2}\n",
      "{'loss': 0.4501, 'learning_rate': 2.987547128838169e-05, 'epoch': 1.21}\n",
      "{'loss': 0.4346, 'learning_rate': 2.9781868829257886e-05, 'epoch': 1.21}\n",
      "{'loss': 0.4533, 'learning_rate': 2.9688266370134077e-05, 'epoch': 1.22}\n",
      "{'loss': 0.4508, 'learning_rate': 2.9594663911010273e-05, 'epoch': 1.22}\n",
      "{'loss': 0.4564, 'learning_rate': 2.9501061451886464e-05, 'epoch': 1.23}\n",
      "{'loss': 0.4591, 'learning_rate': 2.940745899276266e-05, 'epoch': 1.24}\n",
      "{'loss': 0.4548, 'learning_rate': 2.931385653363885e-05, 'epoch': 1.24}\n",
      "{'loss': 0.4283, 'learning_rate': 2.9220254074515046e-05, 'epoch': 1.25}\n",
      "{'loss': 0.451, 'learning_rate': 2.912665161539124e-05, 'epoch': 1.25}\n",
      "{'loss': 0.4031, 'learning_rate': 2.9033049156267433e-05, 'epoch': 1.26}\n",
      "{'loss': 0.4571, 'learning_rate': 2.893944669714363e-05, 'epoch': 1.26}\n",
      "{'loss': 0.4221, 'learning_rate': 2.884584423801982e-05, 'epoch': 1.27}\n",
      "{'loss': 0.4455, 'learning_rate': 2.8752241778896015e-05, 'epoch': 1.27}\n",
      "{'loss': 0.4562, 'learning_rate': 2.8658639319772214e-05, 'epoch': 1.28}\n",
      "{'loss': 0.4343, 'learning_rate': 2.8565036860648402e-05, 'epoch': 1.29}\n",
      "{'loss': 0.4351, 'learning_rate': 2.84714344015246e-05, 'epoch': 1.29}\n",
      "{'loss': 0.434, 'learning_rate': 2.837783194240079e-05, 'epoch': 1.3}\n",
      "{'loss': 0.4336, 'learning_rate': 2.8284229483276985e-05, 'epoch': 1.3}\n",
      "{'loss': 0.4495, 'learning_rate': 2.8190627024153183e-05, 'epoch': 1.31}\n",
      "{'loss': 0.4261, 'learning_rate': 2.809702456502937e-05, 'epoch': 1.31}\n",
      "{'loss': 0.459, 'learning_rate': 2.800342210590557e-05, 'epoch': 1.32}\n",
      "{'loss': 0.4493, 'learning_rate': 2.790981964678176e-05, 'epoch': 1.33}\n",
      "{'loss': 0.4271, 'learning_rate': 2.7816217187657957e-05, 'epoch': 1.33}\n",
      "{'loss': 0.4515, 'learning_rate': 2.7722614728534152e-05, 'epoch': 1.34}\n",
      "{'loss': 0.4322, 'learning_rate': 2.7629012269410344e-05, 'epoch': 1.34}\n",
      "{'loss': 0.4427, 'learning_rate': 2.753540981028654e-05, 'epoch': 1.35}\n",
      "{'loss': 0.4293, 'learning_rate': 2.744180735116273e-05, 'epoch': 1.35}\n",
      "{'loss': 0.4433, 'learning_rate': 2.7348204892038926e-05, 'epoch': 1.36}\n",
      "{'loss': 0.4533, 'learning_rate': 2.7254602432915118e-05, 'epoch': 1.36}\n",
      "{'loss': 0.4441, 'learning_rate': 2.7160999973791313e-05, 'epoch': 1.37}\n",
      "{'loss': 0.4379, 'learning_rate': 2.7067397514667508e-05, 'epoch': 1.38}\n",
      "{'loss': 0.4441, 'learning_rate': 2.69737950555437e-05, 'epoch': 1.38}\n",
      "{'loss': 0.4496, 'learning_rate': 2.6880192596419895e-05, 'epoch': 1.39}\n",
      "{'loss': 0.4468, 'learning_rate': 2.6786590137296087e-05, 'epoch': 1.39}\n",
      "{'loss': 0.4478, 'learning_rate': 2.6692987678172282e-05, 'epoch': 1.4}\n",
      "{'loss': 0.4508, 'learning_rate': 2.6599385219048477e-05, 'epoch': 1.4}\n",
      "{'loss': 0.4471, 'learning_rate': 2.650578275992467e-05, 'epoch': 1.41}\n",
      "{'loss': 0.4592, 'learning_rate': 2.6412180300800864e-05, 'epoch': 1.42}\n",
      "{'loss': 0.4444, 'learning_rate': 2.6318577841677056e-05, 'epoch': 1.42}\n",
      "{'loss': 0.4473, 'learning_rate': 2.622497538255325e-05, 'epoch': 1.43}\n",
      "{'loss': 0.44, 'learning_rate': 2.613137292342945e-05, 'epoch': 1.43}\n",
      "{'loss': 0.4351, 'learning_rate': 2.6037770464305638e-05, 'epoch': 1.44}\n",
      "{'loss': 0.4595, 'learning_rate': 2.5944168005181836e-05, 'epoch': 1.44}\n",
      "{'loss': 0.4375, 'learning_rate': 2.5850565546058025e-05, 'epoch': 1.45}\n",
      "{'loss': 0.4358, 'learning_rate': 2.575696308693422e-05, 'epoch': 1.45}\n",
      "{'loss': 0.439, 'learning_rate': 2.566336062781042e-05, 'epoch': 1.46}\n",
      "{'loss': 0.4357, 'learning_rate': 2.5569758168686607e-05, 'epoch': 1.47}\n",
      "{'loss': 0.4322, 'learning_rate': 2.5476155709562805e-05, 'epoch': 1.47}\n",
      "{'loss': 0.4325, 'learning_rate': 2.5382553250438994e-05, 'epoch': 1.48}\n",
      "{'loss': 0.4474, 'learning_rate': 2.5288950791315192e-05, 'epoch': 1.48}\n",
      "{'loss': 0.4503, 'learning_rate': 2.519534833219138e-05, 'epoch': 1.49}\n",
      "{'loss': 0.4727, 'learning_rate': 2.510174587306758e-05, 'epoch': 1.49}\n",
      "{'loss': 0.4425, 'learning_rate': 2.5008143413943774e-05, 'epoch': 1.5}\n",
      "{'loss': 0.4396, 'learning_rate': 2.4914540954819966e-05, 'epoch': 1.51}\n",
      "{'loss': 0.4249, 'learning_rate': 2.482093849569616e-05, 'epoch': 1.51}\n",
      "{'loss': 0.432, 'learning_rate': 2.4727336036572353e-05, 'epoch': 1.52}\n",
      "{'loss': 0.4419, 'learning_rate': 2.4633733577448548e-05, 'epoch': 1.52}\n",
      "{'loss': 0.4035, 'learning_rate': 2.454013111832474e-05, 'epoch': 1.53}\n",
      "{'loss': 0.4525, 'learning_rate': 2.4446528659200935e-05, 'epoch': 1.53}\n",
      "{'loss': 0.4482, 'learning_rate': 2.435292620007713e-05, 'epoch': 1.54}\n",
      "{'loss': 0.432, 'learning_rate': 2.4259323740953326e-05, 'epoch': 1.54}\n",
      "{'loss': 0.4253, 'learning_rate': 2.4165721281829517e-05, 'epoch': 1.55}\n",
      "{'loss': 0.4276, 'learning_rate': 2.4072118822705712e-05, 'epoch': 1.56}\n",
      "{'loss': 0.4306, 'learning_rate': 2.3978516363581904e-05, 'epoch': 1.56}\n",
      "{'loss': 0.4325, 'learning_rate': 2.38849139044581e-05, 'epoch': 1.57}\n",
      "{'loss': 0.4394, 'learning_rate': 2.3791311445334295e-05, 'epoch': 1.57}\n",
      "{'loss': 0.4318, 'learning_rate': 2.3697708986210486e-05, 'epoch': 1.58}\n",
      "{'loss': 0.446, 'learning_rate': 2.360410652708668e-05, 'epoch': 1.58}\n",
      "{'loss': 0.4297, 'learning_rate': 2.3510504067962873e-05, 'epoch': 1.59}\n",
      "{'loss': 0.4082, 'learning_rate': 2.341690160883907e-05, 'epoch': 1.59}\n",
      "{'loss': 0.4277, 'learning_rate': 2.3323299149715264e-05, 'epoch': 1.6}\n",
      "{'loss': 0.4226, 'learning_rate': 2.3229696690591455e-05, 'epoch': 1.61}\n",
      "{'loss': 0.4276, 'learning_rate': 2.313609423146765e-05, 'epoch': 1.61}\n",
      "{'loss': 0.4406, 'learning_rate': 2.3042491772343842e-05, 'epoch': 1.62}\n",
      "{'loss': 0.4183, 'learning_rate': 2.2948889313220037e-05, 'epoch': 1.62}\n",
      "{'loss': 0.4191, 'learning_rate': 2.285528685409623e-05, 'epoch': 1.63}\n",
      "{'loss': 0.4339, 'learning_rate': 2.2761684394972428e-05, 'epoch': 1.63}\n",
      "{'loss': 0.4047, 'learning_rate': 2.266808193584862e-05, 'epoch': 1.64}\n",
      "{'loss': 0.4491, 'learning_rate': 2.2574479476724815e-05, 'epoch': 1.65}\n",
      "{'loss': 0.4179, 'learning_rate': 2.2480877017601006e-05, 'epoch': 1.65}\n",
      "{'loss': 0.4355, 'learning_rate': 2.23872745584772e-05, 'epoch': 1.66}\n",
      "{'loss': 0.4114, 'learning_rate': 2.2293672099353397e-05, 'epoch': 1.66}\n",
      "{'loss': 0.4383, 'learning_rate': 2.220006964022959e-05, 'epoch': 1.67}\n",
      "{'loss': 0.4275, 'learning_rate': 2.2106467181105784e-05, 'epoch': 1.67}\n",
      "{'loss': 0.4106, 'learning_rate': 2.2012864721981975e-05, 'epoch': 1.68}\n",
      "{'loss': 0.4378, 'learning_rate': 2.191926226285817e-05, 'epoch': 1.68}\n",
      "{'loss': 0.4236, 'learning_rate': 2.1825659803734362e-05, 'epoch': 1.69}\n",
      "{'loss': 0.4298, 'learning_rate': 2.173205734461056e-05, 'epoch': 1.7}\n",
      "{'loss': 0.4204, 'learning_rate': 2.1638454885486753e-05, 'epoch': 1.7}\n",
      "{'loss': 0.4324, 'learning_rate': 2.1544852426362948e-05, 'epoch': 1.71}\n",
      "{'loss': 0.4416, 'learning_rate': 2.145124996723914e-05, 'epoch': 1.71}\n",
      "{'loss': 0.4201, 'learning_rate': 2.1357647508115335e-05, 'epoch': 1.72}\n",
      "{'loss': 0.4499, 'learning_rate': 2.126404504899153e-05, 'epoch': 1.72}\n",
      "{'loss': 0.4257, 'learning_rate': 2.1170442589867722e-05, 'epoch': 1.73}\n",
      "{'loss': 0.4276, 'learning_rate': 2.1076840130743917e-05, 'epoch': 1.74}\n",
      "{'loss': 0.4234, 'learning_rate': 2.098323767162011e-05, 'epoch': 1.74}\n",
      "{'loss': 0.4466, 'learning_rate': 2.0889635212496304e-05, 'epoch': 1.75}\n",
      "{'loss': 0.4426, 'learning_rate': 2.0796032753372496e-05, 'epoch': 1.75}\n",
      "{'loss': 0.4155, 'learning_rate': 2.0702430294248694e-05, 'epoch': 1.76}\n",
      "{'loss': 0.4311, 'learning_rate': 2.0608827835124886e-05, 'epoch': 1.76}\n",
      "{'loss': 0.4238, 'learning_rate': 2.0515225376001078e-05, 'epoch': 1.77}\n",
      "{'loss': 0.4271, 'learning_rate': 2.0421622916877273e-05, 'epoch': 1.77}\n",
      "{'loss': 0.4293, 'learning_rate': 2.0328020457753465e-05, 'epoch': 1.78}\n",
      "{'loss': 0.4271, 'learning_rate': 2.0234417998629663e-05, 'epoch': 1.79}\n",
      "{'loss': 0.447, 'learning_rate': 2.0140815539505855e-05, 'epoch': 1.79}\n",
      "{'loss': 0.4103, 'learning_rate': 2.004721308038205e-05, 'epoch': 1.8}\n",
      "{'loss': 0.4323, 'learning_rate': 1.9953610621258242e-05, 'epoch': 1.8}\n",
      "{'loss': 0.4292, 'learning_rate': 1.9860008162134437e-05, 'epoch': 1.81}\n",
      "{'loss': 0.4152, 'learning_rate': 1.976640570301063e-05, 'epoch': 1.81}\n",
      "{'loss': 0.4155, 'learning_rate': 1.9672803243886824e-05, 'epoch': 1.82}\n",
      "{'loss': 0.4229, 'learning_rate': 1.957920078476302e-05, 'epoch': 1.83}\n",
      "{'loss': 0.4234, 'learning_rate': 1.948559832563921e-05, 'epoch': 1.83}\n",
      "{'loss': 0.4046, 'learning_rate': 1.9391995866515406e-05, 'epoch': 1.84}\n",
      "{'loss': 0.4323, 'learning_rate': 1.9298393407391598e-05, 'epoch': 1.84}\n",
      "{'loss': 0.4321, 'learning_rate': 1.9204790948267796e-05, 'epoch': 1.85}\n",
      "{'loss': 0.4227, 'learning_rate': 1.9111188489143988e-05, 'epoch': 1.85}\n",
      "{'loss': 0.4313, 'learning_rate': 1.9017586030020183e-05, 'epoch': 1.86}\n",
      "{'loss': 0.4505, 'learning_rate': 1.8923983570896375e-05, 'epoch': 1.86}\n",
      "{'loss': 0.4265, 'learning_rate': 1.883038111177257e-05, 'epoch': 1.87}\n",
      "{'loss': 0.4135, 'learning_rate': 1.8736778652648762e-05, 'epoch': 1.88}\n",
      "{'loss': 0.4152, 'learning_rate': 1.8643176193524957e-05, 'epoch': 1.88}\n",
      "{'loss': 0.4253, 'learning_rate': 1.8549573734401152e-05, 'epoch': 1.89}\n",
      "{'loss': 0.4269, 'learning_rate': 1.8455971275277344e-05, 'epoch': 1.89}\n",
      "{'loss': 0.4221, 'learning_rate': 1.836236881615354e-05, 'epoch': 1.9}\n",
      "{'loss': 0.3868, 'learning_rate': 1.826876635702973e-05, 'epoch': 1.9}\n",
      "{'loss': 0.4194, 'learning_rate': 1.817516389790593e-05, 'epoch': 1.91}\n",
      "{'loss': 0.4186, 'learning_rate': 1.808156143878212e-05, 'epoch': 1.92}\n",
      "{'loss': 0.4221, 'learning_rate': 1.7987958979658313e-05, 'epoch': 1.92}\n",
      "{'loss': 0.4041, 'learning_rate': 1.7894356520534508e-05, 'epoch': 1.93}\n",
      "{'loss': 0.4084, 'learning_rate': 1.78007540614107e-05, 'epoch': 1.93}\n",
      "{'loss': 0.4108, 'learning_rate': 1.7707151602286895e-05, 'epoch': 1.94}\n",
      "{'loss': 0.4303, 'learning_rate': 1.761354914316309e-05, 'epoch': 1.94}\n",
      "{'loss': 0.428, 'learning_rate': 1.7519946684039286e-05, 'epoch': 1.95}\n",
      "{'loss': 0.4397, 'learning_rate': 1.7426344224915477e-05, 'epoch': 1.95}\n",
      "{'loss': 0.4188, 'learning_rate': 1.7332741765791672e-05, 'epoch': 1.96}\n",
      "{'loss': 0.4119, 'learning_rate': 1.7239139306667864e-05, 'epoch': 1.97}\n",
      "{'loss': 0.4288, 'learning_rate': 1.714553684754406e-05, 'epoch': 1.97}\n",
      "{'loss': 0.4234, 'learning_rate': 1.7051934388420255e-05, 'epoch': 1.98}\n",
      "{'loss': 0.4111, 'learning_rate': 1.6958331929296446e-05, 'epoch': 1.98}\n",
      "{'loss': 0.4316, 'learning_rate': 1.686472947017264e-05, 'epoch': 1.99}\n",
      "{'loss': 0.4037, 'learning_rate': 1.6771127011048833e-05, 'epoch': 1.99}\n",
      "{'loss': 0.4002, 'learning_rate': 1.667752455192503e-05, 'epoch': 2.0}\n",
      "{'loss': 0.3716, 'learning_rate': 1.6583922092801224e-05, 'epoch': 2.0}\n",
      "{'loss': 0.4077, 'learning_rate': 1.649031963367742e-05, 'epoch': 2.01}\n",
      "{'loss': 0.3839, 'learning_rate': 1.639671717455361e-05, 'epoch': 2.02}\n",
      "{'loss': 0.393, 'learning_rate': 1.6303114715429806e-05, 'epoch': 2.02}\n",
      "{'loss': 0.3716, 'learning_rate': 1.6209512256305997e-05, 'epoch': 2.03}\n",
      "{'loss': 0.3808, 'learning_rate': 1.6115909797182193e-05, 'epoch': 2.03}\n",
      "{'loss': 0.4008, 'learning_rate': 1.6022307338058388e-05, 'epoch': 2.04}\n",
      "{'loss': 0.3825, 'learning_rate': 1.592870487893458e-05, 'epoch': 2.04}\n",
      "{'loss': 0.3784, 'learning_rate': 1.5835102419810775e-05, 'epoch': 2.05}\n",
      "{'loss': 0.3737, 'learning_rate': 1.5741499960686966e-05, 'epoch': 2.06}\n",
      "{'loss': 0.3937, 'learning_rate': 1.564789750156316e-05, 'epoch': 2.06}\n",
      "{'loss': 0.4006, 'learning_rate': 1.5554295042439357e-05, 'epoch': 2.07}\n",
      "{'loss': 0.3811, 'learning_rate': 1.5460692583315552e-05, 'epoch': 2.07}\n",
      "{'loss': 0.3783, 'learning_rate': 1.5367090124191744e-05, 'epoch': 2.08}\n",
      "{'loss': 0.3776, 'learning_rate': 1.5273487665067935e-05, 'epoch': 2.08}\n",
      "{'loss': 0.3986, 'learning_rate': 1.517988520594413e-05, 'epoch': 2.09}\n",
      "{'loss': 0.3815, 'learning_rate': 1.5086282746820324e-05, 'epoch': 2.09}\n",
      "{'loss': 0.4043, 'learning_rate': 1.499268028769652e-05, 'epoch': 2.1}\n",
      "{'loss': 0.388, 'learning_rate': 1.4899077828572713e-05, 'epoch': 2.11}\n",
      "{'loss': 0.3813, 'learning_rate': 1.4805475369448906e-05, 'epoch': 2.11}\n",
      "{'loss': 0.4075, 'learning_rate': 1.47118729103251e-05, 'epoch': 2.12}\n",
      "{'loss': 0.4022, 'learning_rate': 1.4618270451201293e-05, 'epoch': 2.12}\n",
      "{'loss': 0.3768, 'learning_rate': 1.452466799207749e-05, 'epoch': 2.13}\n",
      "{'loss': 0.3985, 'learning_rate': 1.4431065532953683e-05, 'epoch': 2.13}\n",
      "{'loss': 0.3746, 'learning_rate': 1.4337463073829877e-05, 'epoch': 2.14}\n",
      "{'loss': 0.371, 'learning_rate': 1.424386061470607e-05, 'epoch': 2.15}\n",
      "{'loss': 0.3705, 'learning_rate': 1.4150258155582264e-05, 'epoch': 2.15}\n",
      "{'loss': 0.3751, 'learning_rate': 1.4056655696458457e-05, 'epoch': 2.16}\n",
      "{'loss': 0.3762, 'learning_rate': 1.3963053237334652e-05, 'epoch': 2.16}\n",
      "{'loss': 0.3753, 'learning_rate': 1.3869450778210846e-05, 'epoch': 2.17}\n",
      "{'loss': 0.3902, 'learning_rate': 1.377584831908704e-05, 'epoch': 2.17}\n",
      "{'loss': 0.3681, 'learning_rate': 1.3682245859963233e-05, 'epoch': 2.18}\n",
      "{'loss': 0.4009, 'learning_rate': 1.3588643400839426e-05, 'epoch': 2.18}\n",
      "{'loss': 0.3824, 'learning_rate': 1.3495040941715623e-05, 'epoch': 2.19}\n",
      "{'loss': 0.3901, 'learning_rate': 1.3401438482591817e-05, 'epoch': 2.2}\n",
      "{'loss': 0.3679, 'learning_rate': 1.330783602346801e-05, 'epoch': 2.2}\n",
      "{'loss': 0.4034, 'learning_rate': 1.3214233564344204e-05, 'epoch': 2.21}\n",
      "{'loss': 0.3636, 'learning_rate': 1.3120631105220397e-05, 'epoch': 2.21}\n",
      "{'loss': 0.3937, 'learning_rate': 1.3027028646096589e-05, 'epoch': 2.22}\n",
      "{'loss': 0.3813, 'learning_rate': 1.2933426186972786e-05, 'epoch': 2.22}\n",
      "{'loss': 0.3718, 'learning_rate': 1.2839823727848979e-05, 'epoch': 2.23}\n",
      "{'loss': 0.345, 'learning_rate': 1.2746221268725173e-05, 'epoch': 2.24}\n",
      "{'loss': 0.3945, 'learning_rate': 1.2652618809601366e-05, 'epoch': 2.24}\n",
      "{'loss': 0.3882, 'learning_rate': 1.255901635047756e-05, 'epoch': 2.25}\n",
      "{'loss': 0.3913, 'learning_rate': 1.2465413891353755e-05, 'epoch': 2.25}\n",
      "{'loss': 0.3866, 'learning_rate': 1.2371811432229948e-05, 'epoch': 2.26}\n",
      "{'loss': 0.3708, 'learning_rate': 1.2278208973106142e-05, 'epoch': 2.26}\n",
      "{'loss': 0.3746, 'learning_rate': 1.2184606513982335e-05, 'epoch': 2.27}\n",
      "{'loss': 0.3935, 'learning_rate': 1.209100405485853e-05, 'epoch': 2.27}\n",
      "{'loss': 0.3677, 'learning_rate': 1.1997401595734724e-05, 'epoch': 2.28}\n",
      "{'loss': 0.3976, 'learning_rate': 1.1903799136610917e-05, 'epoch': 2.29}\n",
      "{'loss': 0.3765, 'learning_rate': 1.1810196677487112e-05, 'epoch': 2.29}\n",
      "{'loss': 0.3874, 'learning_rate': 1.1716594218363306e-05, 'epoch': 2.3}\n",
      "{'loss': 0.3586, 'learning_rate': 1.16229917592395e-05, 'epoch': 2.3}\n",
      "{'loss': 0.3935, 'learning_rate': 1.1529389300115694e-05, 'epoch': 2.31}\n",
      "{'loss': 0.3919, 'learning_rate': 1.1435786840991886e-05, 'epoch': 2.31}\n",
      "{'loss': 0.3932, 'learning_rate': 1.134218438186808e-05, 'epoch': 2.32}\n",
      "{'loss': 0.3732, 'learning_rate': 1.1248581922744275e-05, 'epoch': 2.33}\n",
      "{'loss': 0.3742, 'learning_rate': 1.1154979463620468e-05, 'epoch': 2.33}\n",
      "{'loss': 0.3647, 'learning_rate': 1.1061377004496663e-05, 'epoch': 2.34}\n",
      "{'loss': 0.3799, 'learning_rate': 1.0967774545372857e-05, 'epoch': 2.34}\n",
      "{'loss': 0.3846, 'learning_rate': 1.087417208624905e-05, 'epoch': 2.35}\n",
      "{'loss': 0.3896, 'learning_rate': 1.0780569627125245e-05, 'epoch': 2.35}\n",
      "{'loss': 0.3847, 'learning_rate': 1.0686967168001439e-05, 'epoch': 2.36}\n",
      "{'loss': 0.359, 'learning_rate': 1.0593364708877632e-05, 'epoch': 2.36}\n",
      "{'loss': 0.3919, 'learning_rate': 1.0499762249753826e-05, 'epoch': 2.37}\n",
      "{'loss': 0.3759, 'learning_rate': 1.040615979063002e-05, 'epoch': 2.38}\n",
      "{'loss': 0.4047, 'learning_rate': 1.0312557331506213e-05, 'epoch': 2.38}\n",
      "{'loss': 0.3868, 'learning_rate': 1.0218954872382408e-05, 'epoch': 2.39}\n",
      "{'loss': 0.3586, 'learning_rate': 1.0125352413258601e-05, 'epoch': 2.39}\n",
      "{'loss': 0.3963, 'learning_rate': 1.0031749954134797e-05, 'epoch': 2.4}\n",
      "{'loss': 0.3785, 'learning_rate': 9.93814749501099e-06, 'epoch': 2.4}\n",
      "{'loss': 0.4142, 'learning_rate': 9.844545035887184e-06, 'epoch': 2.41}\n",
      "{'loss': 0.3876, 'learning_rate': 9.750942576763377e-06, 'epoch': 2.41}\n",
      "{'loss': 0.4009, 'learning_rate': 9.65734011763957e-06, 'epoch': 2.42}\n",
      "{'loss': 0.358, 'learning_rate': 9.563737658515764e-06, 'epoch': 2.43}\n",
      "{'loss': 0.3926, 'learning_rate': 9.470135199391959e-06, 'epoch': 2.43}\n",
      "{'loss': 0.3857, 'learning_rate': 9.376532740268153e-06, 'epoch': 2.44}\n",
      "{'loss': 0.3948, 'learning_rate': 9.282930281144346e-06, 'epoch': 2.44}\n",
      "{'loss': 0.3731, 'learning_rate': 9.189327822020541e-06, 'epoch': 2.45}\n",
      "{'loss': 0.385, 'learning_rate': 9.095725362896735e-06, 'epoch': 2.45}\n",
      "{'loss': 0.3658, 'learning_rate': 9.002122903772928e-06, 'epoch': 2.46}\n",
      "{'loss': 0.4079, 'learning_rate': 8.908520444649123e-06, 'epoch': 2.47}\n",
      "{'loss': 0.396, 'learning_rate': 8.814917985525315e-06, 'epoch': 2.47}\n",
      "{'loss': 0.374, 'learning_rate': 8.72131552640151e-06, 'epoch': 2.48}\n",
      "{'loss': 0.371, 'learning_rate': 8.627713067277704e-06, 'epoch': 2.48}\n",
      "{'loss': 0.3998, 'learning_rate': 8.534110608153897e-06, 'epoch': 2.49}\n",
      "{'loss': 0.3584, 'learning_rate': 8.440508149030092e-06, 'epoch': 2.49}\n",
      "{'loss': 0.404, 'learning_rate': 8.346905689906286e-06, 'epoch': 2.5}\n",
      "{'loss': 0.3857, 'learning_rate': 8.25330323078248e-06, 'epoch': 2.5}\n",
      "{'loss': 0.3777, 'learning_rate': 8.159700771658674e-06, 'epoch': 2.51}\n",
      "{'loss': 0.3587, 'learning_rate': 8.066098312534868e-06, 'epoch': 2.52}\n",
      "{'loss': 0.3992, 'learning_rate': 7.972495853411061e-06, 'epoch': 2.52}\n",
      "{'loss': 0.3565, 'learning_rate': 7.878893394287255e-06, 'epoch': 2.53}\n",
      "{'loss': 0.3941, 'learning_rate': 7.785290935163448e-06, 'epoch': 2.53}\n",
      "{'loss': 0.3751, 'learning_rate': 7.691688476039643e-06, 'epoch': 2.54}\n",
      "{'loss': 0.3844, 'learning_rate': 7.598086016915837e-06, 'epoch': 2.54}\n",
      "{'loss': 0.3881, 'learning_rate': 7.50448355779203e-06, 'epoch': 2.55}\n",
      "{'loss': 0.3784, 'learning_rate': 7.4108810986682255e-06, 'epoch': 2.56}\n",
      "{'loss': 0.386, 'learning_rate': 7.317278639544418e-06, 'epoch': 2.56}\n",
      "{'loss': 0.3726, 'learning_rate': 7.2236761804206115e-06, 'epoch': 2.57}\n",
      "{'loss': 0.3881, 'learning_rate': 7.130073721296807e-06, 'epoch': 2.57}\n",
      "{'loss': 0.3671, 'learning_rate': 7.036471262173e-06, 'epoch': 2.58}\n",
      "{'loss': 0.371, 'learning_rate': 6.942868803049194e-06, 'epoch': 2.58}\n",
      "{'loss': 0.3568, 'learning_rate': 6.849266343925388e-06, 'epoch': 2.59}\n",
      "{'loss': 0.3923, 'learning_rate': 6.755663884801581e-06, 'epoch': 2.59}\n",
      "{'loss': 0.3679, 'learning_rate': 6.6620614256777766e-06, 'epoch': 2.6}\n",
      "{'loss': 0.3528, 'learning_rate': 6.56845896655397e-06, 'epoch': 2.61}\n",
      "{'loss': 0.3819, 'learning_rate': 6.4748565074301635e-06, 'epoch': 2.61}\n",
      "{'loss': 0.3709, 'learning_rate': 6.381254048306358e-06, 'epoch': 2.62}\n",
      "{'loss': 0.3954, 'learning_rate': 6.287651589182551e-06, 'epoch': 2.62}\n",
      "{'loss': 0.3634, 'learning_rate': 6.194049130058746e-06, 'epoch': 2.63}\n",
      "{'loss': 0.3765, 'learning_rate': 6.100446670934939e-06, 'epoch': 2.63}\n",
      "{'loss': 0.3885, 'learning_rate': 6.0068442118111325e-06, 'epoch': 2.64}\n",
      "{'loss': 0.3698, 'learning_rate': 5.913241752687327e-06, 'epoch': 2.65}\n",
      "{'loss': 0.3795, 'learning_rate': 5.819639293563521e-06, 'epoch': 2.65}\n",
      "{'loss': 0.3766, 'learning_rate': 5.726036834439715e-06, 'epoch': 2.66}\n",
      "{'loss': 0.3908, 'learning_rate': 5.632434375315909e-06, 'epoch': 2.66}\n",
      "{'loss': 0.3578, 'learning_rate': 5.538831916192102e-06, 'epoch': 2.67}\n",
      "{'loss': 0.3905, 'learning_rate': 5.445229457068296e-06, 'epoch': 2.67}\n",
      "{'loss': 0.3896, 'learning_rate': 5.35162699794449e-06, 'epoch': 2.68}\n",
      "{'loss': 0.3627, 'learning_rate': 5.2580245388206845e-06, 'epoch': 2.68}\n",
      "{'loss': 0.3836, 'learning_rate': 5.164422079696878e-06, 'epoch': 2.69}\n",
      "{'loss': 0.363, 'learning_rate': 5.070819620573071e-06, 'epoch': 2.7}\n",
      "{'loss': 0.3473, 'learning_rate': 4.977217161449266e-06, 'epoch': 2.7}\n",
      "{'loss': 0.3793, 'learning_rate': 4.88361470232546e-06, 'epoch': 2.71}\n",
      "{'loss': 0.38, 'learning_rate': 4.790012243201654e-06, 'epoch': 2.71}\n",
      "{'loss': 0.3711, 'learning_rate': 4.696409784077847e-06, 'epoch': 2.72}\n",
      "{'loss': 0.3804, 'learning_rate': 4.602807324954041e-06, 'epoch': 2.72}\n",
      "{'loss': 0.3745, 'learning_rate': 4.5092048658302356e-06, 'epoch': 2.73}\n",
      "{'loss': 0.3818, 'learning_rate': 4.415602406706429e-06, 'epoch': 2.74}\n",
      "{'loss': 0.3833, 'learning_rate': 4.321999947582623e-06, 'epoch': 2.74}\n",
      "{'loss': 0.3797, 'learning_rate': 4.228397488458817e-06, 'epoch': 2.75}\n",
      "{'loss': 0.3697, 'learning_rate': 4.134795029335011e-06, 'epoch': 2.75}\n",
      "{'loss': 0.3622, 'learning_rate': 4.041192570211205e-06, 'epoch': 2.76}\n",
      "{'loss': 0.3644, 'learning_rate': 3.947590111087399e-06, 'epoch': 2.76}\n",
      "{'loss': 0.3788, 'learning_rate': 3.853987651963592e-06, 'epoch': 2.77}\n",
      "{'loss': 0.3898, 'learning_rate': 3.7603851928397867e-06, 'epoch': 2.77}\n",
      "{'loss': 0.3696, 'learning_rate': 3.66678273371598e-06, 'epoch': 2.78}\n",
      "{'loss': 0.3524, 'learning_rate': 3.5731802745921745e-06, 'epoch': 2.79}\n",
      "{'loss': 0.3706, 'learning_rate': 3.4795778154683683e-06, 'epoch': 2.79}\n",
      "{'loss': 0.3877, 'learning_rate': 3.385975356344562e-06, 'epoch': 2.8}\n",
      "{'loss': 0.3754, 'learning_rate': 3.2923728972207557e-06, 'epoch': 2.8}\n",
      "{'loss': 0.3632, 'learning_rate': 3.19877043809695e-06, 'epoch': 2.81}\n",
      "{'loss': 0.3683, 'learning_rate': 3.1051679789731435e-06, 'epoch': 2.81}\n",
      "{'loss': 0.3704, 'learning_rate': 3.0115655198493378e-06, 'epoch': 2.82}\n",
      "{'loss': 0.3942, 'learning_rate': 2.9179630607255317e-06, 'epoch': 2.82}\n",
      "{'loss': 0.3534, 'learning_rate': 2.8243606016017256e-06, 'epoch': 2.83}\n",
      "{'loss': 0.362, 'learning_rate': 2.7307581424779195e-06, 'epoch': 2.84}\n",
      "{'loss': 0.3637, 'learning_rate': 2.637155683354113e-06, 'epoch': 2.84}\n",
      "{'loss': 0.362, 'learning_rate': 2.5435532242303072e-06, 'epoch': 2.85}\n",
      "{'loss': 0.377, 'learning_rate': 2.4499507651065007e-06, 'epoch': 2.85}\n",
      "{'loss': 0.3649, 'learning_rate': 2.356348305982695e-06, 'epoch': 2.86}\n",
      "{'loss': 0.3947, 'learning_rate': 2.262745846858889e-06, 'epoch': 2.86}\n",
      "{'loss': 0.3557, 'learning_rate': 2.1691433877350828e-06, 'epoch': 2.87}\n",
      "{'loss': 0.3588, 'learning_rate': 2.0755409286112767e-06, 'epoch': 2.88}\n",
      "{'loss': 0.3821, 'learning_rate': 1.9819384694874706e-06, 'epoch': 2.88}\n",
      "{'loss': 0.3823, 'learning_rate': 1.8883360103636644e-06, 'epoch': 2.89}\n",
      "{'loss': 0.3802, 'learning_rate': 1.7947335512398581e-06, 'epoch': 2.89}\n",
      "{'loss': 0.3788, 'learning_rate': 1.7011310921160522e-06, 'epoch': 2.9}\n",
      "{'loss': 0.3566, 'learning_rate': 1.607528632992246e-06, 'epoch': 2.9}\n",
      "{'loss': 0.386, 'learning_rate': 1.51392617386844e-06, 'epoch': 2.91}\n",
      "{'loss': 0.3765, 'learning_rate': 1.4203237147446337e-06, 'epoch': 2.91}\n",
      "{'loss': 0.3536, 'learning_rate': 1.3267212556208276e-06, 'epoch': 2.92}\n",
      "{'loss': 0.3938, 'learning_rate': 1.2331187964970217e-06, 'epoch': 2.93}\n",
      "{'loss': 0.3686, 'learning_rate': 1.1395163373732156e-06, 'epoch': 2.93}\n",
      "{'loss': 0.3942, 'learning_rate': 1.0459138782494094e-06, 'epoch': 2.94}\n",
      "{'loss': 0.3302, 'learning_rate': 9.523114191256033e-07, 'epoch': 2.94}\n",
      "{'loss': 0.3681, 'learning_rate': 8.587089600017972e-07, 'epoch': 2.95}\n",
      "{'loss': 0.3992, 'learning_rate': 7.651065008779911e-07, 'epoch': 2.95}\n",
      "{'loss': 0.3684, 'learning_rate': 6.71504041754185e-07, 'epoch': 2.96}\n",
      "{'loss': 0.3344, 'learning_rate': 5.779015826303789e-07, 'epoch': 2.97}\n",
      "{'loss': 0.359, 'learning_rate': 4.842991235065728e-07, 'epoch': 2.97}\n",
      "{'loss': 0.3518, 'learning_rate': 3.906966643827667e-07, 'epoch': 2.98}\n",
      "{'loss': 0.3632, 'learning_rate': 2.970942052589606e-07, 'epoch': 2.98}\n",
      "{'loss': 0.3602, 'learning_rate': 2.0349174613515444e-07, 'epoch': 2.99}\n",
      "{'loss': 0.377, 'learning_rate': 1.0988928701134837e-07, 'epoch': 2.99}\n",
      "{'loss': 0.3775, 'learning_rate': 1.628682788754226e-08, 'epoch': 3.0}\n",
      "{'train_runtime': 13365.5742, 'train_samples_per_second': 159.866, 'train_steps_per_second': 19.983, 'train_loss': 0.45210742683711863, 'epoch': 3.0}\n",
      "100% 267087/267087 [3:42:45<00:00, 19.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# do train with newer model on combined SNLI+ANLI train data to see if accuracy improves (Approach A2)\n",
    "!python3 run_SNLI_ANLI.py --do_train --task nli --dataset snli  --output_dir ./results_SNLI_plus_ANLI/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzGOT26xUg4U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Y1u1cFKSQyJ",
    "outputId": "f4119799-69bf-4da4-98dd-bcbdca21c71e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 23:11:23.333739: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 23:11:23.333810: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 23:11:23.333854: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 23:11:24.568462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading builder script: 100% 3.82k/3.82k [00:00<00:00, 16.3MB/s]\n",
      "Downloading metadata: 100% 1.90k/1.90k [00:00<00:00, 9.52MB/s]\n",
      "Downloading readme: 100% 14.1k/14.1k [00:00<00:00, 27.7MB/s]\n",
      "Downloading: 100% 1.93k/1.93k [00:00<00:00, 8.06MB/s]\n",
      "Downloading: 100% 1.26M/1.26M [00:00<00:00, 1.31MB/s]\n",
      "Downloading: 100% 65.9M/65.9M [00:04<00:00, 13.5MB/s]\n",
      "Downloading: 100% 1.26M/1.26M [00:00<00:00, 1.34MB/s]\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Filter: 100% 10000/10000 [00:00<00:00, 117645.69 examples/s]\n",
      "Filter: 100% 550152/550152 [00:05<00:00, 95423.37 examples/s] \n",
      "Filter: 100% 10000/10000 [00:00<00:00, 141026.72 examples/s]\n",
      "Map (num_proc=2): 100% 9842/9842 [00:02<00:00, 4763.07 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 1231/1231 [00:23<00:00, 51.42it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 0.3619251847267151, 'eval_accuracy': 0.8950416445732117, 'eval_runtime': 26.757, 'eval_samples_per_second': 367.828, 'eval_steps_per_second': 46.007}\n"
     ]
    }
   ],
   "source": [
    "# eval snli+anli model on SNLI test (Approach A2)\n",
    "# do eval with new model on SNLI testset\n",
    "!python3 run.py --do_eval --task nli --dataset snli --model ./results_SNLI_plus_ANLI/checkpoint-267000/ --output_dir ./results_eval_SNLIpANLIm_SNLIval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S33ZStH_Siww",
    "outputId": "a60001c1-99a3-45b6-e329-ccd8785140ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 23:42:38.706538: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 23:42:38.706597: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 23:42:38.706634: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 23:42:39.943720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading builder script: 100% 5.55k/5.55k [00:00<00:00, 17.5MB/s]\n",
      "Downloading metadata: 100% 2.76k/2.76k [00:00<00:00, 13.8MB/s]\n",
      "Downloading readme: 100% 7.47k/7.47k [00:00<00:00, 19.2MB/s]\n",
      "Downloading data: 100% 18.6M/18.6M [00:00<00:00, 89.9MB/s]\n",
      "Generating train_r1 split: 100% 16946/16946 [00:01<00:00, 16863.12 examples/s]\n",
      "Generating dev_r1 split: 100% 1000/1000 [00:00<00:00, 14055.88 examples/s]\n",
      "Generating test_r1 split: 100% 1000/1000 [00:00<00:00, 17609.22 examples/s]\n",
      "Generating train_r2 split: 100% 45460/45460 [00:02<00:00, 16811.30 examples/s]\n",
      "Generating dev_r2 split: 100% 1000/1000 [00:00<00:00, 16632.65 examples/s]\n",
      "Generating test_r2 split: 100% 1000/1000 [00:00<00:00, 17207.40 examples/s]\n",
      "Generating train_r3 split: 100% 100459/100459 [00:08<00:00, 12165.55 examples/s]\n",
      "Generating dev_r3 split: 100% 1200/1200 [00:00<00:00, 8760.51 examples/s]\n",
      "Generating test_r3 split: 100% 1200/1200 [00:00<00:00, 9599.60 examples/s]\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Filter: 100% 16946/16946 [00:00<00:00, 75348.96 examples/s]\n",
      "Filter: 100% 1000/1000 [00:00<00:00, 66688.46 examples/s]\n",
      "Filter: 100% 1000/1000 [00:00<00:00, 67994.42 examples/s]\n",
      "Filter: 100% 45460/45460 [00:00<00:00, 96464.49 examples/s]\n",
      "Filter: 100% 1000/1000 [00:00<00:00, 115504.20 examples/s]\n",
      "Filter: 100% 1000/1000 [00:00<00:00, 125958.86 examples/s]\n",
      "Filter: 100% 100459/100459 [00:00<00:00, 165927.29 examples/s]\n",
      "Filter: 100% 1200/1200 [00:00<00:00, 93944.39 examples/s]\n",
      "Filter: 100% 1200/1200 [00:00<00:00, 119857.23 examples/s]\n",
      "Map (num_proc=2): 100% 1000/1000 [00:00<00:00, 2034.70 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 125/125 [00:01<00:00, 62.88it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 1.5962904691696167, 'eval_accuracy': 0.5239999890327454, 'eval_runtime': 2.5496, 'eval_samples_per_second': 392.213, 'eval_steps_per_second': 49.027}\n"
     ]
    }
   ],
   "source": [
    "# eval snli+anli model on ANLI testr1 (Approach A2)\n",
    "# do eval with new model on ANLI testset r1\n",
    "!python3 run_ANLI.py --do_eval --task nli --dataset anli --model ./results_SNLI_plus_ANLI/checkpoint-267000/ --output_dir ./results__eval_SNLIpANLIm_SNLItestr1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2PtUVkvKcW4",
    "outputId": "1b855970-c763-4275-d2d0-c09d0ffeee71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 23:45:36.958221: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 23:45:36.958286: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 23:45:36.958323: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 23:45:38.178503: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100% 1000/1000 [00:00<00:00, 1994.84 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 125/125 [00:02<00:00, 43.00it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 2.0974254608154297, 'eval_accuracy': 0.4309999942779541, 'eval_runtime': 3.7386, 'eval_samples_per_second': 267.483, 'eval_steps_per_second': 33.435}\n"
     ]
    }
   ],
   "source": [
    "# eval snli+anli model on ANLI testr2 (Approach A2)\n",
    "# do eval with new model on ANLI testset r2\n",
    "!python3 run_ANLI.py --do_eval --task nli --dataset anli --model ./results_SNLI_plus_ANLI/checkpoint-267000/ --output_dir ./results__eval_SNLIpANLIm_SNLItestr2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70i8BBmDLHwI",
    "outputId": "5f11ceee-d299-43d7-b019-b2015ff42243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 23:47:29.247568: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 23:47:29.247617: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 23:47:29.247657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 23:47:30.903678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100% 1200/1200 [00:00<00:00, 2229.41 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 150/150 [00:02<00:00, 62.13it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 2.1376211643218994, 'eval_accuracy': 0.41583332419395447, 'eval_runtime': 2.987, 'eval_samples_per_second': 401.737, 'eval_steps_per_second': 50.217}\n"
     ]
    }
   ],
   "source": [
    "# eval snli+anli model on ANLI testr3 (Approach A2)\n",
    "# do eval with new model on ANLI testset r3\n",
    "!python3 run_ANLI.py --do_eval --task nli --dataset anli --model ./results_SNLI_plus_ANLI/checkpoint-267000/ --output_dir ./results__eval_SNLIpANLIm_SNLItestr3/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTjukFHELip1",
    "outputId": "e7f0e477-4503-4574-9812-983039e0bc40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 23:50:17.895292: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 23:50:17.895368: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 23:50:17.895409: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 23:50:19.326000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100% 1000/1000 [00:00<00:00, 2052.77 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 125/125 [00:02<00:00, 62.06it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 1.7939753532409668, 'eval_accuracy': 0.503000020980835, 'eval_runtime': 2.5922, 'eval_samples_per_second': 385.769, 'eval_steps_per_second': 48.221}\n"
     ]
    }
   ],
   "source": [
    "# eval snli+anli model on ANLI devr1 (Approach A2)\n",
    "# do eval with new model on ANLI devr1\n",
    "!python3 run_ANLI.py --do_eval --task nli --dataset anli --model ./results_SNLI_plus_ANLI/checkpoint-267000/ --output_dir ./results__eval_SNLIpANLIm_SNLIdevr1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cl_wtDViMDtx",
    "outputId": "421134af-b02f-465a-e589-882b1b27c0d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 23:51:11.272683: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 23:51:11.272759: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 23:51:11.272806: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 23:51:13.670714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100% 1000/1000 [00:00<00:00, 1978.63 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 125/125 [00:01<00:00, 63.98it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 2.171067953109741, 'eval_accuracy': 0.40700000524520874, 'eval_runtime': 2.5241, 'eval_samples_per_second': 396.178, 'eval_steps_per_second': 49.522}\n"
     ]
    }
   ],
   "source": [
    "# eval snli+anli model on ANLI devr2 (Approach A2)\n",
    "# do eval with new model on ANLI devr2\n",
    "!python3 run_ANLI.py --do_eval --task nli --dataset anli --model ./results_SNLI_plus_ANLI/checkpoint-267000/ --output_dir ./results__eval_SNLIpANLIm_SNLIdevr2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvIAy-2oMY4S",
    "outputId": "f553d7d2-0df6-47fa-da9a-5bd5135e4831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 23:52:18.445135: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 23:52:18.445195: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 23:52:18.445231: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 23:52:19.678913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100% 1200/1200 [00:00<00:00, 2147.29 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 150/150 [00:03<00:00, 44.67it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 2.0600295066833496, 'eval_accuracy': 0.4266666769981384, 'eval_runtime': 3.9266, 'eval_samples_per_second': 305.612, 'eval_steps_per_second': 38.201}\n"
     ]
    }
   ],
   "source": [
    "# eval snli+anli model on ANLI devr3 (Approach A2)\n",
    "# do eval with new model on ANLI devr3\n",
    "!python3 run_ANLI.py --do_eval --task nli --dataset anli --model ./results_SNLI_plus_ANLI/checkpoint-267000/ --output_dir ./results__eval_SNLIpANLIm_SNLIdevr3/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXZc9HVNMpn1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
